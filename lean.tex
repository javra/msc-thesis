My main goal in this thesis project was the formalization and application of
Ronald Brown's structures for non-abelian algebraic topology in the theorem prover
Lean.
Lean, at the point of time when I started working on it, was still in a very early
stage of development and did not only lack any automation but also a basic library
for homotopy type theory.
Thus, we will first take a look at the basic language elements and technologies
used in Lean and then describe the strategies, the structure and the pitfalls
we encountered when building up a library for basic homotopy type theory,
for categories in homotopy type theory, and finally for double groupoids and
crossed modules.

\section{The Lean Theorem Prover}

The development of the theorem prover Lean was initiated in 2013 by Leo\-nar\-do
de Moura and Jeremy Avigad.
De Moura had previously been working on the automated theorem prover Z3, the leading
solver for problem sets in the SMT standard.
With Lean, he intends to create a interactive theorem proving system that connects
the strength of solvers like Z3 with the expressiveness and flexibility of
interactive systems like Agda, Coq or Isabelle.
While in the world of automated theorem proving the verification of a statement
results in a yes-or-no answer at best accompanied by a counterexample in the case
that the statement gets refuted, in interactive theorem proving, we are interested
in an actual proof that a statement is correct.
Since in homotopy type theory it is relevant which proof of a theorem we assume
and since proofs of theorems can be part of another definition or theorem,
the proofs in an interactive theorem prover suitable for homotopy type theory
should even be objects in the language itself.
Lean has two modes: One for standard, proof irrelevant mathematics and one for
homtopy type theory.
In the following, I will only explain the features of latter.

A first ingredient to the language are \textbf{type universes}.
Instead of using $\UU$, universes in Lean are denoted as \leani{Type.{l}}, where
\leani{l} is the level of the universe.
Of course, \leani{Type.{l}} is an object of \leani{Type.{l+1}}.
But in contrast to homotopy type theory as presented in the HoTT book \cite{hottbook},
type universes in Lean are \emph{non-cumulative}, 
i.e. \leani{A : Type.{l}} does not entail \leani{A : Type.{l+1}}.

Definitions can be \emph{universe polymorphic}, which means that, when no concrete
universe levels are given, Lean will keep the definition as general as possible
regarding universe levels of arguments and return type.
The instantiation of a definition \leani{A} at a universe \leani{l} can be received
manually by writing \leani{A.{l}}.
To have manual control over the coherence of universe levels of definitions in
a certain scope, variable universe levels can be declared using the command
\leani{universe variable}. The following snippet shows universe polymorphism
and the use of universe variables:
\begin{leancode}
check Type -- Prints Type.{l_1} : Type.{l_1+1}

universe variable l
check Type.{l} -- Prints Type.{l} : Type.{l+1}
\end{leancode}

The only built-in type formers are (dependent and non-dependent) function types,
structures, and inductive datatypes.

The \textbf{type of functions} between types \leani{A} and \leani{B} is written
as \leani{A → B}.
\leani{A} and \leani{B} do not have to lie in the same universe to form this type
and the universe level of the function type is the maximum of the level of domain
and codomain type.
Lambda abstraction and function application can be written like known from e.g.
Haskell.
$\beta$ reduction is done for each output, $\eta$ conversion is applied when necessary
in the unification process.
\begin{leancode}
variables (A B : Type) (a : A) (b : B) (f : A → B)

check A → B -- Prints A → B : Type.{max l_1 l_2}
check (λ (x : A), b) -- Prints λ (x : A), b : A → B
check (f a) -- Prints f a : B
check (λ x, f x) a -- Prints f a : B
\end{leancode}

An important special case of non-dependent function types are type families
of the form \leani{A → Type}. For every \leani{P : A → Type} we can form the
\textbf{$\Pi$-type} \leani{Π (x : A), P x} over \leani{P}.
Actually, non-dependent function types are just treated as the special case of
dependent functions where \leani{P} is constant.
The $\Pi$-type \leani{Π (x : A), B} for \leani{A B : Type} is automatically reduced
to \leani{A → B}.
\begin{leancode}
variables (A B : Type) (P : A → Type) (Q : Π (x : A) , P x → Type)
variables (p : Π (x : A), P x) (a : A)

check p a -- Prints p a : P a
check Q a -- Prints Q a : P a → Type
check (λ (x : A), Q x (p x)) -- Prints λ (x : A), Q x (p x) : A → Type
\end{leancode}

Lean furthermore allows the definition of inductive types and inductive
families.
To construct an \textbf{inductive type}, one must give a list of parameters the type should
depend on and a list of constructors.
This makes the definition of important types like the natrual numbers or the identity
type possible.
The dependent recursor for inductive types is generated automatically by the kernel:
\begin{leancode}
inductive nat : Type :=
  zero : nat,
  succ : nat → nat
check nat.succ nat.zero) -- Prints nat.succ nat.zero : nat
check @nat.rec_on -- Prints Π {C : nat → Type} (n : nat), C nat.zero →
                  --        (Π (a : nat), C a → C (nat.succ a)) → C n
\end{leancode}

\begin{leancode}
inductive eq (A : Type) (a : A) : A → Type :=
  refl : eq A a a
variables (A : Type) (a : A)
check @eq.refl A a -- Prints eq.refl a : eq A a a
check @eq.rec_on A a -- Prints Π {C : Π (a_1 : A), eq A a a_1 → Type}
                     --        {a_1 : A} (n : eq A a a_1),
                     --        C a (eq.refl a) → C a_1 n
\end{leancode}

We can not only define single inductive types but also \textbf{families of
inductive types}, which we can define by recursion on the index of the family:
\begin{leancode}
open nat

inductive vec (A : Type) : ℕ → Type :=
  nil : vec A 0,
  cons : Π (n : ℕ), A → vec A n → vec A (n+1)

open vec
variables (A : Type) (a : A)
check @vec.rec_on A -- Prints Π {C : Π (a : ℕ), vec A a → Type} {a : ℕ} 
                    --        (n : vec A a), C 0 (nil A) →
                    --        (Π (n : ℕ) (a : A) (a_1 : vec A n), C n a_1
                    --          → C (n + 1) (cons n a a_1)) →
                    --        C a n
check cons 0 a (nil A) -- Prints cons 0 a (nil A) : vec A (0+1)
\end{leancode}

A widely used subclass of inductive types are \textbf{structures}, also often
referred to as "records".
Structures are inductive types which are non-recursive and only have one constructor.
That means that they are equivalent to iterated sigma types but, among other advantages,
have named projections.
Structures provide a basic inheritance mechanism as they can extend arbitrarily
many other structures with \textbf{coercions} being added for each parent structure:
\begin{leancode}
structure graph (V : Type₀) :=
  (E : V → V → Type₀)

structure refl_graph (V : Type₀) extends graph V :=
  (refl : Π (v : V), E v v)

structure trans_graph (V : Type₀) extends graph V :=
  (trans : Π (u v w : V), E u v → E v w → E u w)

structure refl_trans_graph (V : Type₀) extends refl_graph V, trans_graph V

variables (V : Type₀)
check graph.E (refl_graph.mk (λ (a b : V), a = b) eq.refl)
/- Prints
  graph.E (refl_graph.to_graph 
    (refl_graph.mk (λ (a b : V), a = b) eq.refl)) :
    V → V → Type₀ -/
\end{leancode}

As one can already seen in these small examples, writing out the full names and
the complete list of parameters for each call of a defined function can be very
tedious.
Lean implements some features that allow its users to make theory files more
succinct and readable by leaving out information that can be inferred automatically
by Lean.

One feature that allows more brevity are \textbf{implicit arguments}.
To mark an argument to a definition to be automatically inferred by Lean,
the user can mark it with curly brackets instead of round brackets when listing
it in the signatur of the definition.
Of course the missing arguments have to be such that they can be uniquely determined
by the unification process, otherwise the unifier will return an appropriate error
message whenever the definition is used.
The user can make all arguments in a function call implicit by prepending a
\leani{!} to the definition name.
The opposite, making all arguments explicit, can be achieved by prepending the
symbol \leani{@}.
By default, implicit arguments are \emph{maximally inserted}.
That means, that any expression of a $\Pi$-type with its first argument implicit
is not interpreted as is but further applied by inference of that argument.
By using double curly brackets \leani{⦃...⦄} instead of single curly brackts,
the user can change this behaviour to be more passive by only inferring the argument
if it precedes a explicitely stated one:
\begin{leancode}
definition inverse {P : Type} {x y : P} (p : x = y) : y = x :=
  eq.rec (eq.refl x) p
check inverse -- Prints inverse : ?x = ?y → ?y = ?x

definition id ⦃A : Type⦄ (a : A) := A
check id -- Prints id : Π ⦃A : Type⦄, A → Type

variables {A : Type} (a : A)
check id a -- Prints id a : Type
\end{leancode}

Another useful feature to organize formalizations and shorten code is the interaction
between namespaces and overloading.
In general there are no restrictions on \textbf{overloading} theorem names but each additional
overload with the same name makes it harder for the elaborator to figure out which
one it needs to use in a certain case.
To make overloads more organized, definitions can be put in hierarchical \textbf{namespaces}
which can be opened selectively.
Opening a namespace makes each theorem in that namespace available to be called
with its name as stated in the definition inside that namespace, as opposed to
giving its absolute name.
Definitions can be marked as \leani{protected} to prevent their names from being
pruned and as \leani{private} to exclude them from exporting completely.
Furthermore, opening namespaces enables the use of \textbf{notations} defined
in that namespace.
\begin{leancode}
open nat

namespace exponentiation
  definition squared (a : ℕ) := a * a
  notation a`²`:100 := squared a
end exponentiation

open exponentiation
check squared -- Prints squared : ℕ → ℕ
eval 4² -- Prints 16
\end{leancode}
Often, consecutive definitions share a lot of arguments.
To avoid the need to repeat those arguments for each definition, Lean provides
\textbf{sections} and \textbf{contexts} which serve as scopes for common variables.
Variables declared in sections is persistent while variables in contexts will not
be accessible after closing the scope.
Besides \leani{variables}, there are \leani{parameters} which will not be generalized
until the context or section is closed.

One last feature helping to create short and readable files are \textbf{type
classes}.
After marking an inductive type or a family of inductive type as \leani{[class]},
we can declare definitions that are objects of this type as \leani{[instance]}.
Then, we can use a fourth mode of argument implicitness (besides \leani{(...)},
\leani{{...}}, and \leani{⦃...⦄}) denoted by \leani{[...]} to tell Lean that it
should infer this argument by filling in one of the instances of the required
type.
The selection of one out of several fitting instances can be influenced by assigning
priorities to the instance declarations.
Type classes are a tool for achieving \emph{canonicity}.
One important application is the fixation of instances of algebraic structures on
certain types or their closure under certain type formers, for example the structure
of an abelian group on the type of integers or the direct product of groups as
a canonical group structure on a product type.
By marking theorems as instances which themselves have type class parameters,
we can put quite some automation load on the type class resolution.
In the following example we give a canonical graph structure to the natural numbers
and prove that the resulting graph is serial:
\begin{leancode}
open nat eq sigma.ops

structure graph [class] (V : Type₀) := (E : V → V → Type₀)

definition nat_graph [instance] : graph nat := graph.mk (λ x y, y = x+1)

definition is_serial (V : Type₀) [G : graph V] := Π x, Σ y, graph.E x y

definition nat_serial : is_serial nat := λ x, ⟨x+1, idp⟩
\end{leancode}

In all previous examples, we stated all definitions and proofs by giving the
whole term at once.
In some theorem provers like Agda \cite{agda}, this method, support by the presence
of \texttt{let} and \texttt{where} terms, is used for all proofs while others,
like Coq \cite{coq}, rely on the use of \textbf{tactics}.
Tactics are sequentially executed commands that transform a given proof goal into
an easier to solve set of goals.
Lean allows both approaches to be used purely or even using tactics for subterms
of a declarative proofs.
The user can switch to the tactic mode with \leani{begin ... end}.
As soon as the tactic block has been successfully filled in with tactic calls,
the full proof term gets elaborated, type checked, and is from there on in\-
distinguishable from a directly entered proof.

The tactics in Lean are still under development and will be extended in near
future.
Thus, I will only give a list of the tactics that were used at the time of
writing the formalizations in this thesis:
\begin{itemize}
\item The tactic \leani{exact} takes a proof term as argument which it uses
to solve the first subgoal completely.
If Lean fails to infer every argument that is not stated explicitly or if Lean
cannot unify the expression with the goal, the tactic fails.
\item If one wants to treat the current first subgoal with an arbitrary theorem
without already solving it completely, one can use \leani{apply}.
It unifies the theorem with the subgoal but opposed to failing it creates a new
subgoal for each undetermined or unresolved argument of the state theorem.
The new subgoals are added from the last argument to the first.
Arguments, that can be inferred from later arguments will not be converted to a
new subgoal.
\item Sometimes, one wants to prove premises or give arguments in the order
they appear in the statement of the applied theorem.
This can be achieved by using \leani{fapply}.
In general, it results in more subgoals than \leani{apply}.
\item Using \leani{assert} lets the user proof a named statement which is added
to the context for the rest of the main proof.
\item If the current subgoal is a (dependent or non-dependent) function, \leani{intro}
adds the variable, which the goal quantifies over, to the context and applies it
to the goal. \leani{intros} does the same iteratively for multiple variables.
\leani{revert} does the opposite: It selects a variable in the context and
replaces the goal by its quantification over this variable.
A similar job is done by \leani{generalize}.
It takes an arbitrary term as argument and quantifies the goal over a variable
that term's type, replacing all its occurrences by the quantifying variable.
\item The tactic \leani{clear} is used to delete unused variables from the context.
This is especially useful to speed up type class resolution and to shorten the output
of the current context.
\item \leani{cases} destructs a given context variable of an inductive type 
using the automatically generated \leani{cases_on} theorem.
Optionally, a list of the desired names for the newly generated variables can be
appended.
\item Often, a goal is not expressed in its simplest form.
\leani{esimp} simplifies the goal after unfolding a given list of definitions.
\item The \leani{rewrite} tactic takes a theorem yielding a (propositional)
equality and replaces occurrences of the equation's left hand side by the 
right hand side.
The user can specify in what pattern or how often the replacement should be made.
\item By using \leani{rotate}, one can rotate the list of subgoals by a given
number of steps.
This is useful when the user wants to postpone the goal at the first position
to be solved at the end of the proof.
\item Tactics can furthermore be concatenated to a single tactic using the and-then
composition \leani{( tactic1 ; tactic2 )}.
The resulting tactic fails if either one of the tactics fail. \\
The notation \leani{[ tactic1 | tactic2 ]} specifies an or-else branching of tactics:
Lean first tries to apply the first tactic and if it fails the second one.
The composed tactic only fails if both of them fail.
\item Adding to this composition of tactics one can use the \leani{repeat}
tactic as a loop command.
The tactic given to \leani{repeat} as an argument is applied over and over until
it fails.
So, by giving it an or-else composition of tactics Lean tries each one from a set
of tactics until none of them is applicable.
\item Curly brackets can be used to create a scope which only aims to solve the
first subgoal.
If the subgoal is not solved within that scope, an error will appear at the line
of the closing bracket. %TODO make this sound better
\end{itemize} %TODO give some examples maybe?

\section{Basic Homotopy Type Theory in Lean}

Lean's homotopy type theory library is split into two parts:
The content of a folder \texttt{init} is imported by default and provides
theories for built-in features like \leani{rewrite}.
Besides this folder there are other theories which are not required at startup
and which the user can import manually.
They include characterizations of path spaces and basic theories about common
algebraic structures which will be presented in the next chapter.
The extension and maintenance of the library is an ongoing joint effort by
Jeremy Avigad, Floris van Doorn, Leonardo de Moura and me.

One important definition in the initialization folder is, of course, the one of
\textbf{propositional equality}.
As mentioned above, equality is just a very basic example for an inductive type.
Here, we define equality as inductive family of types \leani{eq} and with \leani{idp}
give an alternative name for $\refl$ with the base being implicit:
\begin{leancode}
universe variable l
inductive eq.{l} {A : Type.{l}} (a : A) : A → Type.{l} :=
refl : eq a a

definition idp {a : A} := refl a
\end{leancode}
Concatenation and inversion of paths are defined using path induction and abbreviated
with the obvious notation:
\begin{leancode}
definition concat (p : x = y) (q : y = z) : x = z := eq.rec (λu, u) q p
definition inverse (p : x = y) : y = x := eq.rec (refl x) p

notation p₁ ⬝ p₂ := concat p₁ p₂
notation p ⁻¹ := inverse p
\end{leancode}
By this definition, $p \ct \refl$ is definitionally equal to $p$ while $\refl \ct p = p$
can only be proved by induction on $p$.
A lot of basic calculations in the path groupoid can also be proved by just using
path induction. Her is one example for this kind of lemma:
\begin{leancode}
definition eq_of_idp_eq_inv_con (p q : x = y) : idp = p⁻¹ ⬝ q → p = q :=
eq.rec_on p (take q h, h ⬝ (idp_con _)) q
\end{leancode}
Two other basic definitions for paths are the one of transports and $\ap$.
\begin{leancode}
definition transport [reducible] (P : A → Type) {x y : A}
  (p : x = y) (u : P x) : P y :=
eq.rec_on p u

definition ap ⦃A B : Type⦄ (f : A → B) {x y:A} (p : x = y) : f x = f y :=
eq.rec_on p idp
\end{leancode}

To be able to express univalence it is important to have a useful definition of
the \textbf{equivalence of types}.
We use structures to express the type of proofs $\isequiv(f)$ that a function $f$
is a half-adjoint equivalence as well as for the type of equivalences between
two given types (\textasciitilde \, denotes the type of homotopies between two functions) :
\begin{leancode}
structure is_equiv [class] {A B : Type} (f : A → B) :=
  (inv : B → A)
  (retr : (f ∘ inv) ∼ id)
  (sect : (inv ∘ f) ∼ id)
  (adj : Π x, retr (f x) = ap f (sect x))

structure equiv (A B : Type) :=
  (to_fun : A → B)
  (to_is_equiv : is_equiv to_fun)
\end{leancode}
In practice, we will almost never use the generated constructor of \leani{is_equiv}
but instead we proved an alternative constructor \leani{adjointify} which does
not require the adjointness proof \leani{adj}.

\textbf{Univalence} than states that the function \leani{equiv_of_eq}, which lets
us gain an equivalence from an equality between two types in the same universe,
is itself an equivalence.
We mark the axiom as instance so that, whenever there is mention of \leani{equiv_of_eq},
we have access to its inverse.
This inverse \leani{ua} is also the only common form univalence appears elsewhere.
\begin{leancodebr}
section
  universe variable l
  variables {A B : Type.{l}}

  definition is_equiv_tr_of_eq (H : A = B) :
    is_equiv (transport (λ X, X) H) :=
  @is_equiv_tr Type (λX, X) A B H

  definition equiv_of_eq (H : A = B) : A ≃ B :=
  equiv.mk _ (is_equiv_tr_of_eq H)
end

axiom univalence (A B : Type) : is_equiv (@equiv_of_eq A B)
attribute univalence [instance]

definition ua {A B : Type} : A ≃ B → A = B := (@equiv_of_eq A B)⁻¹
\end{leancodebr}

We use the univalence axiom to prove \textbf{function extensionality} first of
non-dependent and then of dependent functions.
We first define three varieties of function extensionality and then proof that
the desired definition of function extensionality (stating that the function
\leani{apD10} which returns for each equality between functions a homotopy
between those functions) follows from the other ones.
This approach has been ported from the Coq HoTT library
~\footnote{\url{https://github.com/HoTT/HoTT}}.
\begin{leancode}
definition funext.{l k} := 
Π ⦃A : Type.{l}⦄ {P : A → Type.{k}} (f g : Π x, P x),
  is_equiv (@apD10 A P f g)

-- Naive funext is the assertion that pointwise equal functions are equal.
definition naive_funext :=
Π ⦃A : Type⦄ {P : A → Type} (f g : Πx, P x), (f ∼ g) → f = g

-- Weak funext says that a product of contractible types is contractible.
definition weak_funext :=
Π ⦃A : Type⦄ (P : A → Type) [H: Πx, is_contr (P x)], is_contr (Πx, P x)
\end{leancode}

\textbf{Truncation levels} are implemented by first creating a version of the natural
numbers that ``start at -2'' together with a coercion from the actual type
\leani{nat}, then defining internal versions of contractability and truncatedness,
and at eventually defining a type class structure holding a proof of the internal
truncatedness:
\begin{leancodebr}
inductive trunc_index : Type₁ :=
  minus_two : trunc_index,
  succ : trunc_index → trunc_index
...
structure contr_internal (A : Type) :=
  (center : A)
  (contr : Π(a : A), center = a)

definition is_trunc_internal (n : trunc_index) : Type → Type :=
trunc_index.rec_on n (λA, contr_internal A)
  (λn trunc_n A, (Π(x y : A), trunc_n (x = y)))
...
structure is_trunc [class] (n : trunc_index) (A : Type) :=
  (to_internal : is_trunc_internal n A)

abbreviation is_contr := is_trunc -2
abbreviation is_hprop := is_trunc -1
abbreviation is_hset  := is_trunc 0
\end{leancodebr}
By this trick we can then define contractability as the special case of being 
truncated at level -2.
This makes it a lot easier to write theorems that hold for all levels of
truncatedness.
The type class \leani{is_trunc} has many instances that, by calling type class
resolution themselves, automatize the process of finding the truncation level
of a given iterated $\Sigma$-type or $\Pi$-type (proofs and surrounding
context omitted, definitions gathered from multiple files):
\begin{leancodebr}
--Equalities in contractible types are contractible.
definition is_contr_eq {A : Type} [H : is_contr A] (x y : A) :
  is_contr (x = y) := ...

-- n-types are also (n+1)-types.
definition is_trunc_succ [instance] [priority 100] 
  (A : Type) (n : trunc_index) [H : is_trunc n A] : is_trunc (n.+1) A := ...

-- The unit type is contractible.
definition is_contr_unit [instance] : is_contr unit := ...

-- The empty type is a mere proposition.
definition is_hprop_empty [instance] : is_hprop empty := ...

-- A Sigma type is n-truncated if the type of every possible projections is.
definition is_trunc_sigma [instance] (B : A → Type) (n : trunc_index)
  [HA : is_trunc n A] [HB : Πa, is_trunc n (B a)] :
  is_trunc n (Σ a, B a) := ...
  
-- Any dependent product of n-types is an n-type.
definition is_trunc_pi [instance] (B : A → Type) (n : trunc_index)
  [H : Πa, is_trunc n (B a)] : is_trunc n (Πa, B a) := ...
  
-- Being an equivalence is a mere proposition.
theorem is_hprop_is_equiv [instance] : is_hprop (is_equiv f) := ...
\end{leancodebr}

An important part of the library besides the initialization files consists of
theorems characterizing paths between instances of different type formers.
One example for such a characterization is that a path between two dependent
pairs can be built out of paths between their projections:
\begin{leancode}
definition dpair_eq_dpair (p : a = a') (q : p ▹ b = b') : ⟨a, b⟩ = ⟨a', b'⟩ :=
by cases p; cases q; apply idp

definition sigma_eq (p : u.1 = v.1) (q : p ▹ u.2 = v.2) : u = v :=
by cases u; cases v; apply (dpair_eq_dpair p q)
\end{leancode}

\begin{table}[p]
\begin{center}
\begin{tabular}{l|r|r}
\toprule[1pt]
\multicolumn{1}{c}{Theory} 
	& \multicolumn{1}{c}{Line Count} 
	& \multicolumn{1}{c}{Compilation Time in s} \\ 
\midrule[1pt]
\leani{init.} & & \\
	\hspace{1em}\leani{bool} & 28 & 0.043\\
	\hspace{1em}\leani{datatypes} & 90 & 0.044\\
	\hspace{1em}\leani{default} & 15 & 0.421\\
	\hspace{1em}\leani{equiv} & 276 & 1.082\\
	\hspace{1em}\leani{function} & 61 & 0.042\\
	\hspace{1em}\leani{hedberg} & 47 & 0.399\\
	\hspace{1em}\leani{logic} & 359 & 0.173\\
	\hspace{1em}\leani{nat} & 345 & 0.565\\
	\hspace{1em}\leani{num} & 135 & 0.073\\
	\hspace{1em}\leani{path} & 648 & 2.683\\
	\hspace{1em}\leani{priority} & 12 & 0.036\\
	\hspace{1em}\leani{relation} & 43 & 0.072\\
	\hspace{1em}\leani{reserved_notation} & 103 & 0.035\\
	\hspace{1em}\leani{tactic} & 106 & 0.067\\
	\hspace{1em}\leani{trunc} & 262 & 0.821\\
	\hspace{1em}\leani{util} & 18 & 0.324\\
	\hspace{1em}\leani{wf} & 162 & 0.394\\
	\hspace{1em}\leani{axioms.} & & \\
		\hspace{2em}\leani{funext_of_ua} & 162 & 0.674\\
		\hspace{2em}\leani{funext_varieties} & 111 & 0.483\\
		\hspace{2em}\leani{ua} & 51 & 0.302\\
	\hspace{1em}\leani{types.} & & \\
		\hspace{2em}\leani{empty} & 23 & 0.055\\
		\hspace{2em}\leani{prod} & 99 & 0.225\\
		\hspace{2em}\leani{sigma} & 26 & 0.058\\
		\hspace{2em}\leani{sum} & 19 & 0.036\\ %TODO repeat this without -t 0
\bottomrule[1pt]
\end{tabular}
\caption{Theories imported in Lean's initial startup.} \label{tab:init-tree}
\end{center}
\end{table} 

\begin{table}[p]
\begin{center}
\begin{tabular}{l|r|r}
\toprule[1pt]
\multicolumn{1}{c}{Theory} 
	& \multicolumn{1}{c}{Line Count} 
	& \multicolumn{1}{c}{Compilation Time in s} \\ 
\midrule[1pt]
\leani{arity} & 188 & 0.797\\
\leani{algebra.} & & \\
	\hspace{1em}\leani{binary} & 74 & 0.156\\
	\hspace{1em}\leani{group} & 570 & 1.317\\
	\hspace{1em}\leani{relation} & 122 & 0.330\\
\leani{types.} & & \\
	\hspace{1em}\leani{arrow} & 49 & 0.143\\
	\hspace{1em}\leani{eq} & 271 & 0.556\\
	\hspace{1em}\leani{equiv} & 98 & 0.652\\
	\hspace{1em}\leani{fiber} & 51 & 0.199\\
	\hspace{1em}\leani{pi} & 198 & 1.177\\
	\hspace{1em}\leani{pointed} & 40 & 0.121\\
	\hspace{1em}\leani{prod} & 48 & 0.144\\
	\hspace{1em}\leani{sigma} & 397 & 1.599\\
	\hspace{1em}\leani{trunc} & 140 & 0.371\\
	\hspace{1em}\leani{W} & 157 & 0.128\\
\bottomrule[1pt]
\end{tabular}
\caption{Theories in Lean's standard library for its homotopy type theory mode.
(Category theory excluded.)} \label{tab:hottlib-tree}
\end{center}
\end{table}

Tables \ref{tab:init-tree} and \ref{tab:hottlib-tree} show the line count and
the compilation time for each theory in Lean's HoTT library excluding the library
for category theory.

\section{Category Theory in Lean}

Our library of basic category theoretical definitions and theorems, which is
still work in progress, also aims to mimic the structure of Coq's HoTT implementation
of categories while being more succinct than it.
We use structures and type classes for most definitions of algebraic structures
and their closure properties.

The central structure our formalization revolves around is, of course, the one
of a precategory:
\begin{leancode}
structure precategory [class] (ob : Type) : Type :=
  (hom : ob → ob → Type)
  (homH : Π(a b : ob), is_hset (hom a b))
  (comp : Π⦃a b c : ob⦄, hom b c → hom a b → hom a c)
  (ID : Π (a : ob), hom a a)
  (assoc : Π ⦃a b c d : ob⦄ (h : hom c d) (g : hom b c) (f : hom a b),
     comp h (comp g f) = comp (comp h g) f)
  (id_left : Π ⦃a b : ob⦄ (f : hom a b), comp !ID f = f)
  (id_right : Π ⦃a b : ob⦄ (f : hom a b), comp f !ID = f)
\end{leancode}

Since usually the domain of an identity morphism is determined by the context
we will most often use a shorter notation for the identity:
\begin{leancode}
definition id [reducible] := ID a
\end{leancode}

We introduce a shortcut for the type class instance postulating that each equality
of morphism is a mere proposition.
We use this instance, for example, to prove that it is sufficient to give equalities
between the morphism types, composition and identities of two precategories on the
same type of objects, to show that these precategories are equal:

\begin{leancode}
definition is_hprop_eq_hom [instance] : is_hprop (f = f') := !is_trunc_eq

definition precategory_eq_mk' (ob : Type) (C D : precategory ob)
  (p : @hom ob C = @hom ob D)
  (q : transport (λ x, Πa b c, x b c → x a b → x a c) p
    (@comp ob C) = @comp ob D)
  (r : transport (λ x, Πa, x a a) p (@ID ob C) = @ID ob D) : C = D :=
begin
  cases C, cases D,
  apply precategory_eq_mk, apply q, apply r,
end
\end{leancode}

We also define a \emph{bundled} version of a category, as the structure containing
the object type and a precategory on it as fields:
\begin{leancode}
structure Precategory : Type :=
  (carrier : Type)
  (struct : precategory carrier)
\end{leancode}

Using typeclasses for split monos, split epis and isomorphisms enables us to access
the left, right or both-sided inverse filling in the precategory structure as
well as the invertability witness by type class instance resolution:
\begin{leancode}
structure split_mono [class]
    {ob : Type} [C : precategory ob] {a b : ob} (f : a ⟶ b) :=
  {retraction_of : b ⟶ a}
  (retraction_comp : retraction_of ∘ f = id)

structure split_epi [class]
    {ob : Type} [C : precategory ob] {a b : ob} (f : a ⟶ b) :=
  {section_of : b ⟶ a}
  (comp_section : f ∘ section_of = id)

structure is_iso [class]
    {ob : Type} [C : precategory ob] {a b : ob} (f : a ⟶ b) :=
  {inverse : b ⟶ a}
  (left_inverse  : inverse ∘ f = id)
  (right_inverse : f ∘ inverse = id)
\end{leancode}

On purpose, we weaken some theorems to accepting arbitrary witnesses for mere propositions
that would actually be derivable from the other witnesses.
By this, we can have class instance resolution fill in a witness selected by the
highest instance priority.
In the following example, even if there is a theorem obtaining \leani{is_iso (f⁻¹)}
from \leani{is_iso f}, we might want the witness to be the axiom that in a groupoid
all morphisms are isomorphisms.
This spares us the need of transporting the statement to the right witness each
time we use it:
\begin{leancode}
definition inverse_involutive (f : a ⟶ b) [H : is_iso f] [H : is_iso (f⁻¹)]
  : (f⁻¹)⁻¹ = f :=
inverse_eq_right !left_inverse

definition id_inverse (a : ob) [H : is_iso (ID a)] : (ID a)⁻¹ = id :=
inverse_eq_left !id_comp

definition comp_inverse [Hp : is_iso p] [Hpq : is_iso (q ∘ p)] :
  (q ∘ p)⁻¹ʰ = p⁻¹ʰ ∘ q⁻¹ʰ :=
inverse_eq_left (show (p⁻¹ʰ ∘ q⁻¹ʰ) ∘ q ∘ p = id, from
  by rewrite [-assoc, inverse_comp_cancel_left, left_inverse])
\end{leancode}

Not only the the property of a morphism $f$ being an isomorphism but also the type
of morphisms between two given objects is encapsulated in a structure that heavily
helps to fill in proofs automatically. %TODO more precise
\begin{leancode}
structure iso (a b : ob) :=
  (to_hom : hom a b)
  [struct : is_iso to_hom]

infix `≅`:50 := iso.iso
attribute iso.struct [instance] [priority 400]

-- The type of isomorphisms between two objects is a set.
definition is_hset_iso [instance] : is_hset (a ≅ b) :=
begin
  apply is_trunc_is_equiv_closed,
    apply (equiv.to_is_equiv (!iso.sigma_char)),
end
\end{leancode}

Another definition we want to generalize to the two dimensional case is the one
of a functor.
We introduce functors as a structure and add coercions to its function on objects
and to its function on morphisms.
By doing so, we will be able to write \leani{F a} for the image of an object \leani{a}
and \leani{F f} for the image of a morphism \leani{f} under a functor \leani{F}.
\begin{leancode}
structure functor (C D : Precategory) : Type :=
  (to_fun_ob : C → D)
  (to_fun_hom : Π ⦃a b : C⦄, hom a b → hom (to_fun_ob a) (to_fun_ob b))
  (respect_id : Π (a : C), to_fun_hom (ID a) = ID (to_fun_ob a))
  (respect_comp : Π {a b c : C} (g : hom b c) (f : hom a b),
    to_fun_hom (g ∘ f) = to_fun_hom g ∘ to_fun_hom f)

infixl `⇒`:25 := functor
attribute to_fun_ob [coercion]
attribute to_fun_hom [coercion]
\end{leancode}

One example where these coercions are used is the definition of the composition
of functors:
\begin{leancode}
definition compose [reducible] (G : functor D E) (F : functor C D) :
  functor C E :=
functor.mk
  (λ x, G (F x))
  (λ a b f, G (F f))
  (λ a, calc
    G (F (ID a)) = G (ID (F a)) : by rewrite respect_id
             ... = ID (G (F a)) : by rewrite respect_id)
  (λ a b c g f, calc
    G (F (g ∘ f)) = G (F g ∘ F f)     : by rewrite respect_comp
              ... = G (F g) ∘ G (F f) : by rewrite respect_comp)
\end{leancode}

Like mentioned above, it is useful to still have a representation of a structure
as an iterated product and $\Sigma$-type.
With this characterization we can formalize lemma \ref{thm:functors-hset} easily:
\begin{leancode}
protected definition sigma_char :
  (Σ (to_fun_ob : C → D)
  (to_fun_hom : Π ⦃a b : C⦄, hom a b → hom (to_fun_ob a) (to_fun_ob b)),
  (Π (a : C), to_fun_hom (ID a) = ID (to_fun_ob a)) ×
  (Π {a b c : C} (g : hom b c) (f : hom a b),
    to_fun_hom (g ∘ f) = to_fun_hom g ∘ to_fun_hom f)) ≃ (functor C D) :=
...

set_option apply.class_instance false
protected definition is_hset_functor [HD : is_hset D] : is_hset (C ⇒ D) :=
begin
  apply is_trunc_is_equiv_closed, apply equiv.to_is_equiv,
    apply sigma_char,
  apply is_trunc_sigma, apply is_trunc_pi, intros, exact HD, intro F,
  apply is_trunc_sigma, apply is_trunc_pi, intro a,
    {apply is_trunc_pi, intro b,
     apply is_trunc_pi, intro c, apply !homH},
  intro H, apply is_trunc_prod,
    {apply is_trunc_pi, intro a,
     apply is_trunc_eq, apply is_trunc_succ, apply !homH},
    {repeat (apply is_trunc_pi; intros),
     apply is_trunc_eq, apply is_trunc_succ, apply !homH},
end
\end{leancode}

This enables us to implement the precategory of strict precategory (compare
corollary \ref{thm:precat-strict-precat}).
The proof that this precategory is univalent (lemma \ref{thm:precat-univalent})
still needs a formalization.

\begin{leancode}
structure strict_precategory [class] (ob : Type) extends precategory ob :=
  (is_hset_ob : is_hset ob)

structure Strict_precategory : Type :=
  (carrier : Type)
  (struct : strict_precategory carrier)

definition precat_strict_precat : precategory Strict_precategory :=
precategory.mk (λ a b, functor a b)
  (λ a b, @functor.is_hset_functor a b _)
  (λ a b c g f, functor.compose g f)
  (λ a, functor.id)
  (λ a b c d h g f, !functor.assoc)
  (λ a b f, !functor.id_left)
  (λ a b f, !functor.id_right)
\end{leancode}

\begin{table}[h]
\begin{center}
\begin{tabular}{l|r|r}
\toprule[1pt]
\multicolumn{1}{c}{Theory} 
	& \multicolumn{1}{c}{Line Count} 
	& \multicolumn{1}{c}{Compilation Time in s} \\ 
\midrule[1pt]
\leani{algebra.} & & \\
	\hspace{1em}\leani{groupoid} & 119 & 0.419\\
	\hspace{1em}\leani{category.} & & \\
		\hspace{2em}\leani{basic} & 74 & 0.265 \\
		\hspace{2em}\leani{constructions} & 144 & 1.180 \\
	\hspace{1em}\leani{precategory.} &  & \\
		\hspace{2em}\leani{adjoints} & 143 & *0.638 \\
		\hspace{2em}\leani{basic} & 236 & 1.870 \\
		\hspace{2em}\leani{constructions} & 268 & 1.862 \\
		\hspace{2em}\leani{functor} & 253 & *2.981 \\
		\hspace{2em}\leani{iso} & 350 & 2.131 \\
		\hspace{2em}\leani{nat_trans} & 114 & 1.033 \\
		\hspace{2em}\leani{strict} & 53 & 0.246	\\
		\hspace{2em}\leani{yoneda} & 208 & 3.852 \\
\bottomrule[1pt]
\end{tabular}
\caption{The theories in Lean's category theory library.
Theories marked with * contain unfinished proofs.} \label{tab:cat-tree}
\end{center}
\end{table}

\section{Formalizing Double Groupoids}

When formalizing the structures presented in chapters \ref{chapter:nat} and
\ref{chapter:types}, I proceeded in the order the concepts are presented in this
thesis.
Since it was necessary to change several definitions to improve compatibility with
the library and to improve performance I had to change most of the definitions
repeatedly during the process.
In the following, I will only present the final version of the definitions.

The first structure is the one of a \textbf{double category}.
Since it has many fields, it was useful to come up with an idea to shorten the definition:
We first define what it means to be a \textbf{worm category}.
This structure consists of objects, morphisms and two-cells the same way a double
category does, but it only allows for composition in one direction.
In contrast to an actual double category there are essentially two, instead of
three, categories involved in the definition of a worm category.
\begin{leancodebr}
structure worm_precat {D₀ : Type} (C  : precategory D₀)
  (D₂ : Π ⦃a b c d : D₀⦄
    (f : hom a b) (g : hom c d) (h : hom a c) (i : hom b d), Type) :=
  (comp₁ : proof Π ⦃a b c₁ d₁ c₂ d₂ : D₀⦄
    ⦃f₁ : hom a b⦄ ⦃g₁ : hom c₁ d₁⦄ ⦃h₁ : hom a c₁⦄ ⦃i₁ : hom b d₁⦄
    ⦃g₂ : hom c₂ d₂⦄ ⦃h₂ : hom c₁ c₂⦄ ⦃i₂ : hom d₁ d₂⦄,
    (D₂ g₁ g₂ h₂ i₂) → (D₂ f₁ g₁ h₁ i₁)
    → (@D₂ a b c₂ d₂ f₁ g₂ (h₂ ∘ h₁) (i₂ ∘ i₁)) qed)
  (ID₁ : proof Π ⦃a b : D₀⦄ (f : hom a b), D₂ f f (ID a) (ID b) qed)
  (assoc₁ : proof Π ⦃a b c₁ d₁ c₂ d₂ c₃ d₃ : D₀⦄
    ⦃f  : hom a b⦄   ⦃g₁ : hom c₁ d₁⦄ ⦃h₁ : hom a c₁⦄ ⦃i₁ : hom b d₁⦄
    ⦃g₂ : hom c₂ d₂⦄ ⦃h₂ : hom c₁ c₂⦄ ⦃i₂ : hom d₁ d₂⦄
    ⦃g₃ : hom c₃ d₃⦄ ⦃h₃ : hom c₂ c₃⦄ ⦃i₃ : hom d₂ d₃⦄
    (w : D₂ g₂ g₃ h₃ i₃) (v : D₂ g₁ g₂ h₂ i₂) (u : D₂ f g₁ h₁ i₁),
    (assoc i₃ i₂ i₁) ▹ ((assoc h₃ h₂ h₁) ▹
        (comp₁ w (comp₁ v u))) = (comp₁ (comp₁ w v) u) qed)
  (id_left₁ : proof Π ⦃a b c d : D₀⦄
    ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄
    (u : D₂ f g h i),
    (id_left i) ▹ ((id_left h) ▹ (comp₁ (ID₁ g) u)) = u qed)
  (id_right₁ : proof Π ⦃a b c d : D₀⦄
    ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄
    (u : D₂ f g h i),
    (id_right i) ▹ ((id_right h) ▹ (comp₁ u (ID₁ f))) = u qed)
  (homH' : proof Π ⦃a b c d : D₀⦄
    ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄,
    is_hset (D₂ f g h i) qed)
\end{leancodebr}

We then use the inheritance mechanism for structures to define a double category
as extending two worm precategories on the same object type \leani{D₀} and 1-skeleton
\leani{C} and a dependent type of two-cells that differ by \emph{transposition}
in the sense that if \leani{D₂} is the type of two-cells of the ``vertical''
worm category, \leani{(λ ⦃a b c d : D₀⦄ f g h i, D₂ h i f g)} is the respective
dependent type for the ``horizontal'' one.
To prevent the fields of the two worm precategories from being merged (which is
the default for structure fields with identical names), we have to rename
the fields of the horizontal worm category.
Then we add the laws that could not be expressed in terms of only one direction
of two-cell composition.
\begin{leancode}
structure dbl_precat {D₀ : Type} (C : precategory D₀)
  (D₂ : Π ⦃a b c d : D₀⦄ (f : hom a b) (g : hom c d) (h : hom a c) (i : hom b d),
    Type)
  extends worm_precat C D₂,
    worm_precat C (λ ⦃a b c d : D₀⦄ f g h i, D₂ h i f g)
  renaming comp₁→comp₂ ID₁→ID₂ assoc₁→assoc₂
    id_left₁→id_left₂ id_right₁→id_right₂ homH'→homH'_dontuse :=
  (id_comp₁ : proof Π {a b c : D₀} (f : hom a b) (g : hom b c),
    ID₂ (g ∘ f) = comp₁ (ID₂ g) (ID₂ f) qed)
  (id_comp₂ : proof Π {a b c : D₀} (f : hom a b) (g : hom b c),
    ID₁ (g ∘ f) = comp₂ (ID₁ g) (ID₁ f) qed)
  (zero_unique : proof Π (a : D₀), ID₁ (ID a) = ID₂ (ID a) qed)
  (interchange : proof Π {a₀₀ a₀₁ a₀₂ a₁₀ a₁₁ a₁₂ a₂₀ a₂₁ a₂₂ : D₀}
    {f₀₀ : hom a₀₀ a₀₁} {f₀₁ : hom a₀₁ a₀₂} {f₁₀ : hom a₁₀ a₁₁}
    {f₁₁ : hom a₁₁ a₁₂} {f₂₀ : hom a₂₀ a₂₁} {f₂₁ : hom a₂₁ a₂₂}
    {g₀₀ : hom a₀₀ a₁₀} {g₀₁ : hom a₀₁ a₁₁} {g₀₂ : hom a₀₂ a₁₂}
    {g₁₀ : hom a₁₀ a₂₀} {g₁₁ : hom a₁₁ a₂₁} {g₁₂ : hom a₁₂ a₂₂}
    (x : D₂ f₁₁ f₂₁ g₁₁ g₁₂) (w : D₂ f₁₀ f₂₀ g₁₀ g₁₁)
    (v : D₂ f₀₁ f₁₁ g₀₁ g₀₂) (u : D₂ f₀₀ f₁₀ g₀₀ g₀₁),
    comp₁ (comp₂ x w) (comp₂ v u) = comp₂ (comp₁ x v) (comp₁ w u) qed)
\end{leancode}

This unbundled definition of a precategory is useful when making statements about
all double category structures on a certain pair of 1-skeleton and two-cell type.
But since we often want to talk about \emph{all} double categories, we add a bundled
up version that also adds the strictness condition. %TODO make that consistent
\begin{leancode}
structure Dbl_precat : Type :=
  (cat : Precategory)
  (two_cell : Π ⦃a b c d : cat⦄ (f : hom a b)
    (g : hom c d) (h : hom a c) (i : hom b d), Type)
  (struct : dbl_precat cat two_cell)
  (obj_set : is_hset (carrier cat))
\end{leancode}

With that definition, we can start instantiating first simple examples.
When implementing the square double category (definition \ref{def:shell-dbl-cat-hott}),
we can make use of the \leani{repeat} tactic to fill in each of the many arguments
of the double category constructor with either the constructor $\star$ of $\unit$, the
fact that $\unit$ is a mere proposition or the fact mere propositions are sets:
\begin{leancode}
definition square_dbl_precat : dbl_precat C
  (λ ⦃a b c d : D₀⦄ (f : hom a b) (g : hom c d)
    (h : hom a c) (i : hom b d), unit) :=
begin
  fapply dbl_precat.mk,
    repeat (intros; (rexact ⋆ |  apply is_hprop.elim | apply is_trunc_succ)),
    repeat (intros;  apply idp),
end
\end{leancode}

The definition of the shell double category is not as straightforward, since we
have to perform the calculations from \ref{def:shell-dbl-cat} for the commutativity
of composite squares:
\begin{leancodebr}
definition comm_square_dbl_precat : dbl_precat C
  (λ ⦃a b c d : D₀⦄ (f : hom a b) (g : hom c d)
    (h : hom a c) (i : hom b d), g ∘ h = i ∘ f) :=
begin
  fapply dbl_precat.mk,
    intros, exact (calc g₂ ∘ h₂ ∘ h₁ = (g₂ ∘ h₂) ∘ h₁ : assoc
                                ... = (i₂ ∘ g₁) ∘ h₁ : a_1
                                ... = i₂ ∘ g₁ ∘ h₁ : assoc
                                ... = i₂ ∘ i₁ ∘ f₁ : a_2
                                ... = (i₂ ∘ i₁) ∘ f₁ : assoc),
    intros, exact (calc f ∘ ID a = f : id_right
                             ... = ID b ∘ f : id_left),
    repeat (intros; apply is_hset.elim),
    intros, apply is_trunc_eq,
    intros, exact (calc (i₂ ∘ i₁) ∘ f₁ = i₂ ∘ i₁ ∘ f₁ : assoc
                                   ... = i₂ ∘ g₁ ∘ h₁ : a_2
                                   ... = (i₂ ∘ g₁) ∘ h₁ : assoc
                                   ... = (g₂ ∘ h₂) ∘ h₁ : a_1
                                   ... = g₂ ∘ h₂ ∘ h₁ : assoc),
    intros, exact (calc ID b ∘ f = f : id_left
                             ... = f ∘ ID a : id_right),
    repeat (intros; apply is_hset.elim),
    intros, apply is_trunc_eq,
    repeat (intros; apply is_hset.elim),
end
\end{leancodebr}

Extracting the horizontal and vertical precategory of a double category is
something that can already be defined on a worm precategory where we have \emph{one}
category of two-cells.
We encapsulate the objects and the morphisms of this category in their own
structure definitions:
\begin{leancode}
parameters {D₀ : Type} [C : precategory D₀] {D₂ : ...} (D : worm_precat C D₂)

structure two_cell_ob :=
  (vo1 vo2 : D₀)
  (vo3 : hom vo1 vo2)

structure two_cell_connect (Sf Sg : two_cell_ob) :=
  (vc1 : hom (two_cell_ob.vo1 Sf) (two_cell_ob.vo1 Sg))
  (vc2 : hom (two_cell_ob.vo2 Sf) (two_cell_ob.vo2 Sg))
  (vc3 : D₂ (two_cell_ob.vo3 Sf) (two_cell_ob.vo3 Sg) vc1 vc2)
\end{leancode}

After characterizing those types by sigma types, characterizing equalities between
them and examining the truncation level of the structures we can define the
two-cell precategory of a worm category: (Note the relations between the universe
levels involved.)
\begin{leancodebr}
universe variables l₀ l₁ l₂
variables {D₀ : Type.{l₀}} [C : precategory.{l₀ (max l₀ l₁)} D₀]
  {D₂ : Π ..., Type.{max l₀ l₁ l₂}}

definition two_cell_precat (D : worm_precat C D₂)
  : precategory.{(max l₀ l₁) (max l₀ l₁ l₂)} (two_cell_ob D) :=
begin
  fapply precategory.mk.{(max l₀ l₁) (max l₀ l₁ l₂)},
    intros [Sf, Sg], exact (two_cell_connect D Sf Sg),
    intros [Sf, Sg], apply is_trunc_is_equiv_closed, apply equiv.to_is_equiv,
      exact (two_cell_connect_sigma_char D Sf Sg),
      apply is_trunc_sigma, intros,
      apply is_trunc_sigma, intros, apply (homH' D),
    intros [Sf, Sg, Sh, Sv, Su], apply (two_cell_comp D Sv Su),
    intro Sf, exact (two_cell_id D Sf),
    intros, exact (two_cell_assoc D h g f),
    intros [Sf, Sg, Su], exact (two_cell_id_left D Su),
    intros [Sf, Sg, Su], exact (two_cell_id_right D Su),
end
\end{leancodebr}

Now we can receive the vertical and horizontal precategory as the two-cell
precategory of each parent worm precategory:
\begin{leancode}
definition vert_precat (D : dbl_precat C D₂) :=
worm_precat.two_cell_precat.{l₀ l₁ l₂} (to_worm_precat_1 D)

definition horiz_precat (D : dbl_precat C D₂) :=
worm_precat.two_cell_precat.{l₀ l₁ l₂} (to_worm_precat_2 D)
\end{leancode}

Before we continue to define thin structures and double groupoids,
we create a library of helper lemma that will be used in the other theories
over and over again.
In the definition of a double category we already saw that often composite
squares have to be transported along an equality in one of their faces.
Continuing to prove more complex equations of squares, these transports can
best be managed when on the \emph{outside} of a composition.
So we create a library of theorems that equate a square composition involving
a transport on one of both of the squares with a another composition where the
transport is pulled to the outside or eliminated.
Some examples for these lemmas are the following:
\begin{leancodebr}
variables {a b c d b₂ d₂ : D₀} {E : Type}
  {f : hom a b} {g : hom c d} {h : hom a c} {i : hom b d}
  {f₂ : hom b b₂} {g₂ : hom d d₂} {i₂ : hom b₂ d₂}

definition transp_comp₂_eq_comp₂_transp_l_l {e : E → hom a c}
  {h h' : E} (q : h = h')
  (u : D₂ f g (e h) i) (v : D₂ f₂ g₂ i i₂) :
  transport (λ x, D₂ _ _ (e x) _) q (comp₂ D v u)
  = comp₂ D v (transport (λ x, D₂ _ _ (e x) _) q u) :=
by cases q; apply idp

definition transp_comp₂_inner_deal1 {e : E → hom b d}
  {i i' : E} (q : i = i')
  (u : D₂ f g h (e i)) (v : D₂ f₂ g₂ (e i') i₂) :
  comp₂ D v (transport (λ x, D₂ _ _ _ (e x)) q u)
  = comp₂ D (transport (λ x, D₂ _ _ (e x) _) q⁻¹ v) u :=
by cases q; apply idp

definition transp_comp₂_eq_comp₂_transp_inner {e : E → hom b d}
  {i i' : E} (q : i = i')
  (u : D₂ f g h (e i)) (v : D₂ f₂ g₂ (e i) i₂) :
  comp₂ D v u = comp₂ D (transport (λ x, D₂ _ _ (e x) _) q v)
    (transport (λ x, D₂ _ _ _ (e x)) q u) :=
by cases q; apply idp
\end{leancodebr}

Since for a given double category there will, in all relevant cases, be one
canonical thin structure, we use the type class mechanism to find that thin
structure as an instance of the following \textbf{type class of thin structures}:
\begin{leancodebr}
structure thin_structure [class] {D₀ : Type} [C : precategory D₀]
    {D₂ : Π ⦃a b c d : D₀⦄, hom a b → hom c d → hom a c → hom b d → Type}
    (D : dbl_precat C D₂) :=
  (thin : Π ⦃a b c d : D₀⦄
    (f : hom a b) (g : hom c d) (h : hom a c) (i : hom b d), g ∘ h = i ∘ f
    → D₂ f g h i)
  (thin_id₁ : proof Π ⦃a b : D₀⦄ (f : hom a b),
    thin f f (ID a) (ID b) ((id_right f) ⬝ (id_left f)⁻¹) = ID₁ D f qed)
  (thin_id₂ : proof Π ⦃a b : D₀⦄ (f : hom a b),
    thin (ID a) (ID b) f f ((id_left f) ⬝ (id_right f)⁻¹) = ID₂ D f qed)
  (thin_comp₁ : proof Π ⦃a b c₁ d₁ c₂ d₂ : D₀⦄
    ⦃f₁ : hom a b⦄ ⦃g₁ : hom c₁ d₁⦄ ⦃h₁ : hom a c₁⦄ ⦃i₁ : hom b d₁⦄
    ⦃g₂ : hom c₂ d₂⦄ ⦃h₂ : hom c₁ c₂⦄ ⦃i₂ : hom d₁ d₂⦄
    (pv : g₂ ∘ h₂ = i₂ ∘ g₁) (pu : g₁ ∘ h₁ = i₁ ∘ f₁)
    (px : g₂ ∘ h₂ ∘ h₁ = (i₂ ∘ i₁) ∘ f₁),
    comp₁ D (thin g₁ g₂ h₂ i₂ pv) (thin f₁ g₁ h₁ i₁ pu)
    = thin f₁ g₂ (h₂ ∘ h₁) (i₂ ∘ i₁) px qed)
  (thin_comp₂ : proof Π ⦃a b c₁ d₁ c₂ d₂ : D₀⦄
    ⦃f₁ : hom a b⦄ ⦃g₁ : hom c₁ d₁⦄ ⦃h₁ : hom a c₁⦄ ⦃i₁ : hom b d₁⦄
    ⦃g₂ : hom c₂ d₂⦄ ⦃h₂ : hom c₁ c₂⦄ ⦃i₂ : hom d₁ d₂⦄
    (pv : i₂ ∘ g₁ = g₂ ∘ h₂) (pu : i₁ ∘ f₁ = g₁ ∘ h₁)
    (px : (i₂ ∘ i₁) ∘ f₁ = g₂ ∘ h₂ ∘ h₁),
    comp₂ D (thin h₂ i₂ g₁ g₂ pv) (thin h₁ i₁ f₁ g₁ pu)
    = thin (h₂ ∘ h₁) (i₂ ∘ i₁) f₁ g₂ px qed)

open thin_structure
check @thin_id₁ /- Prints
  thin_id₁ :
  Π {D₀ : Type} {C : precategory D₀}
    {D₂ : Π ⦃a b c d : D₀⦄, hom a b → hom c d → hom a c → hom b d → Type}
    (D : dbl_precat C D₂) [c : thin_structure D] ⦃a b : D₀⦄ (f : hom a b),
        thin D f f (ID a) (ID b) (id_right f ⬝ (id_left f)⁻¹) = ID₁ D f -/
\end{leancodebr}

Defining a connection is, of course, very straightforward:
\begin{leancode}
definition br_connect ⦃a b : D₀⦄ (f : hom a b) : D₂ f (ID b) f (ID b) :=
thin D f (ID b) f (ID b) idp

definition ul_connect ⦃a b : D₀⦄ (f : hom a b) : D₂ (ID a) f (ID a) f :=
thin D (ID a) f (ID a) f idp
\end{leancode}

In contrast, proving the S-law (\ref{thm:s-law})
takes surprisingly much effort.
It is the first of many theorems for which we need helper lemmas that
generalize paths to make a statement provable by path induction.
\begin{leancodebr}
definition ID₁_of_ul_br_aux {a b : D₀} (f g h : hom a b)
  (p : g = f) (q : h = f)
  (r1 : h ∘ id = id ∘ g) (r2 : f ∘ id = id ∘ f)
  (rr : q ▹ (p ▹ r1) = r2) :
  q ▹ (p ▹ thin D g h id id r1) = thin D f f id id r2 :=
by cases rr; cases p; cases q; apply idp

definition ID₁_of_ul_br ⦃a b : D₀⦄ (f : hom a b) :
  (id_left f) ▹ ((id_right f) ▹
  (comp₂ D (br_connect f) (ul_connect f))) = ID₁ D f :=
begin
  -- Bring transports to right hand side
  apply tr_eq_of_eq_inv_tr, apply tr_eq_of_eq_inv_tr,
  -- Work on left hand side
  apply concat,
    -- Composites of thin squares are thin
    apply thin_comp₂,
    -- Commutativity of composite square
    apply inverse, apply assoc,
  -- Bring transports to left hand side
  apply eq_inv_tr_of_tr_eq, apply eq_inv_tr_of_tr_eq,
  apply concat,
    -- Apply helper lemma eliminating transports
    apply ID₁_of_ul_br_aux, apply is_hset.elim,
    exact ((id_right f) ⬝ (id_left f)⁻¹),
  -- Identity squares are thin
  apply thin_id₁,
end
\end{leancodebr}

To prove the transport laws, we use a similar auxiliary lemma and use the
\leani{assert} command to provide proofs of commutativity and  for the different
rows to the context:
\begin{leancodebr}
  definition br_of_br_square_aux {a c : D₀} (gf : hom a c)
    (h₁ : hom c c) (p : h₁ = ID c)
    (r1 : h₁ ∘ gf = h₁ ∘ gf) (r2 : (ID c) ∘ gf = (ID c) ∘ gf) :
    (p ▹ thin D gf h₁ gf h₁ r1) = thin D gf (ID c) gf (ID c) r2 :=
  by cases p; apply (ap (λ x, thin D _ _ _ _ x) !is_hset.elim)

  definition br_of_br_square ⦃a b c : D₀⦄ (f : hom a b) (g : hom b c) :
    (id_left id) ▹ (comp₁ D (comp₂ D (br_connect g) (ID₂ D g))
      (comp₂ D (ID₁ D g) (br_connect f)))
    = br_connect (g ∘ f) :=
  begin
    apply tr_eq_of_eq_inv_tr,
    -- Prove commutativity of second row
    assert line2_commute : (id ∘ id) ∘ g = id ∘ g ∘ id,
      exact (calc (id ∘ id) ∘ g = id ∘ g : @id_left D₀ C
                           ... = (id ∘ g) ∘ id : id_right
                           ... = id ∘ (g ∘ id) : assoc),
    -- Prove thinness of second row
    assert line2_thin : comp₂ D (br_connect g) (ID₂ D g)
      = thin D (g ∘ id) (id ∘ id) g id line2_commute,
      apply concat, apply (ap (λx, comp₂ D _ x)), apply inverse, apply thin_id₂,
      apply thin_comp₂,
    -- Prove commutativity of first row
    assert line1_commute : (g ∘ id) ∘ f = id ∘ g ∘ f,
      exact (calc (g ∘ ID b) ∘ f = g ∘ f : @id_right D₀ C
                            ... = ID c ∘ g ∘ f : id_left),
    -- Prove thinness of first row
    assert line1_thin : comp₂ D (ID₁ D g) (br_connect f)
      = thin D (g ∘ f) (g ∘ id) f id line1_commute,
      apply concat, apply (ap (λx, comp₂ D x _)), apply inverse, apply thin_id₁,
      apply thin_comp₂,
    -- Replace composite squares by thin squares
    apply concat, exact (ap (λx, comp₁ D x _) line2_thin),
    apply concat, exact (ap (λx, comp₁ D _ x) line1_thin),
    -- Thinness of the entire 2x2 grid
    apply concat, apply thin_comp₁, apply idp,
    apply eq_inv_tr_of_tr_eq,
    apply br_of_br_square_aux,
  end
\end{leancodebr}

Now we can start defining double groupoids.
The definition of a weak double groupoid poses a greater task to lean's type
checker than the one of a double category which is why we first define the the
type of the arguments that are required to make a weak double groupoid out of a
double category as separate definitions to give Lean the chance to elaborate
and type check the terms before using them in the actual definition.
Note that in the \leani{extends} command in structure definition, \leani{C} is
automatically casted from a groupoid to a general precategory.

\begin{leancodebr}
context
  parameters
    {D₀ : Type}
    (C : groupoid D₀)
    (D₂ : Π ⦃a b c d : D₀⦄, hom a b → hom c d → hom a c →  hom b d → Type)
    (D : dbl_precat C D₂)

  definition inv₁_type : Type :=
  Π ⦃a b c d : D₀⦄ {f : hom a b} {g : hom c d} {h : hom a c} {i : hom b d},
    D₂ f g h i → D₂ g f (h⁻¹) (i⁻¹)

  definition left_inverse₁_type (inv₁ : inv₁_type) : Type :=
  Π ⦃a b c d : D₀⦄ {f : hom a b} {g : hom c d} {h : hom a c} {i : hom b d}
    (u : D₂ f g h i),
    (left_inverse i) ▹ (left_inverse h) ▹ (comp₁ D (inv₁ u) u) = ID₁ D f
  ...
end

structure weak_dbl_gpd {D₀ : Type} (C : groupoid D₀)
  (D₂ : Π ⦃a b c d : D₀⦄, hom a b → hom c d → hom a c →  hom b d → Type)
  extends D : dbl_precat C D₂ :=
  (inv₁ : inv₁_type C D₂)
  (left_inverse₁ : left_inverse₁_type C D₂ D inv₁)
  (right_inverse₁ : right_inverse₁_type C D₂ D inv₁)
  (inv₂ : inv₂_type C D₂)
  (left_inverse₂ : left_inverse₂_type C D₂ D inv₂)
  (right_inverse₂ : right_inverse₂_type C D₂ D inv₂)
\end{leancodebr}

To implement and get the definition~\ref{def:dbl-gpd-hott} of a double
group\-oid we only have to add a thin structure to a weak double groupoid.
For the definition of the category of double group\-oids we again add a strict and
bundled version:
\begin{leancodebr}
structure dbl_gpd {D₀ : Type} (C : groupoid D₀) (D₂ : Π ⦃a b c d : D₀⦄, 
    hom a b → hom c d → hom a c →  hom b d → Type)
  extends D : weak_dbl_gpd C D₂:=
  (T : thin_structure (weak_dbl_gpd.to_dbl_precat D))

structure Dbl_gpd : Type :=
  (gpd : Groupoid)
  (two_cell : Π ⦃a b c d : gpd⦄,
    hom a b → hom c d → hom a c →  hom b d → Type)
  (struct : dbl_gpd gpd two_cell)
  (obj_set : is_hset (carrier gpd))
\end{leancodebr}

For the definition of a double functor we again have to pull out the definition
of the argument types to make it easier for the elaborator:

\begin{leancodebr}
  structure dbl_functor (D E : Dbl_gpd) :=
    (catF : functor (gpd D) (gpd E))
    (twoF : Π ⦃a b c d : gpd D⦄
      ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄,
      two_cell D f g h i → two_cell E (catF f) (catF g) (catF h) (catF i))
    (respect_id₁ : respect_id₁_type D E catF twoF)
    (respect_comp₁ : respect_comp₁_type D E catF twoF)
    (respect_id₂ : respect_id₂_type D E catF twoF)
    (respect_comp₂ : respect_comp₂_type D E catF twoF)
\end{leancodebr}

As a first lemma characterizing equalities between double functors we have the
following:

\begin{leancodebr}
  parameters (D E : Dbl_gpd)
    (catF1 catF2 : functor (gpd D) (gpd E))
    (twoF1 : Π ⦃a b c d : gpd D⦄
      ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄,
      two_cell D f g h i → two_cell E (catF1 f) (catF1 g) (catF1 h) (catF1 i))
    (twoF2 : Π ⦃a b c d : gpd D⦄
      ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄,
      two_cell D f g h i → two_cell E (catF2 f) (catF2 g) (catF2 h) (catF2 i))
    (respect_id₁1 : proof respect_id₁_type D E catF1 qed twoF1)
    (respect_id₁2 : proof respect_id₁_type D E catF2 qed twoF2)
    (respect_comp₁1 : proof respect_comp₁_type D E catF1 qed twoF1)
    (respect_comp₁2 : proof respect_comp₁_type D E catF2 qed twoF2)
    (respect_id₂1 : proof respect_id₂_type D E catF1 qed twoF1)
    (respect_id₂2 : proof respect_id₂_type D E catF2 qed twoF2)
    (respect_comp₂1 : proof respect_comp₂_type D E catF1 qed twoF1)
    (respect_comp₂2 : proof respect_comp₂_type D E catF2 qed twoF2) 

  definition dbl_functor.congr (p1 : catF1 = catF2) (p2 : p1 ▹ twoF1 = twoF2) :
    dbl_functor.mk catF1 twoF1 
      respect_id₁1 respect_comp₁1 respect_id₂1 respect_comp₂1
    = dbl_functor.mk catF2 twoF2 
      respect_id₁2 respect_comp₁2 respect_id₂2 respect_comp₂2 :=
  begin
    cases p1, cases p2,
    intros, apply (ap01111 (λ f g h i, dbl_functor.mk catF2 twoF2 f g h i)),
      repeat (
        repeat ( apply eq_of_homotopy ; intros ) ;
        apply (@is_hset.elim _ (!(homH' E))) ),
  end
\end{leancodebr}

But in practice, this formalization turned out to be less useful than one where
we don't have a path between the category functors but instead between their
objects and morphisms, separately:
\begin{leancodebr}
    (p1 : to_fun_ob catF1 = to_fun_ob catF2)
    (p2 : transport
      (λ x, Π (a b : carrier (gpd D)), hom a b → hom (x a) (x b)) p1
      (to_fun_hom catF1) = to_fun_hom catF2)
    (p3 : apD011 (λ Hob Hhom,
                  Π ⦃a b c d : carrier (gpd D)⦄
                    ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄,
                    two_cell D f g h i →
                    @two_cell E (Hob a) (Hob b) (Hob c) (Hob d)
                     (Hhom a b f) (Hhom c d g) (Hhom a c h) (Hhom b d i))
          p1 p2 ▹ twoF1 = twoF2)
\end{leancodebr}

Using these more fine grained prerequisites for equality between double functors
we can prove the associativity of double functors in a pretty straightforward way:
\begin{leancodebr}
  definition dbl_functor.assoc {B C D E : Dbl_gpd}
    (H : dbl_functor D E) (G : dbl_functor C D) (F : dbl_functor B C) :
    dbl_functor.compose H (dbl_functor.compose G F)
    = dbl_functor.compose (dbl_functor.compose H G) F :=
  begin
    fapply (dbl_functor.congr' B E),
        apply idp,
      apply idp,
    apply idp,
  end
\end{leancodebr}

The composition mentioned in that theorem is defined as obvious on the 1-skeleton
and on the two-cells, it is quite a bit of work to show that it respects
identities and composition.
Let's take a look at the part of the proof that shows that the composite functor
respects the vertical identity:
\begin{leancodebr}
      intros, apply tr_eq_of_eq_inv_tr, apply tr_eq_of_eq_inv_tr,
      apply concat, apply (ap (λ x, twoF G x)), apply respect_id₁',
      apply concat, apply twoF_transport_l, apply tr_eq_of_eq_inv_tr,
      apply concat, apply twoF_transport_r, apply tr_eq_of_eq_inv_tr,
      apply concat, apply respect_id₁',
      apply inv_tr_eq_of_eq_tr, apply inv_tr_eq_of_eq_tr,
      apply inverse,
      apply concat, apply (transport_eq_transport4 (λ f g h i, two_cell E f g h i)),
      apply concat, apply transport4_transport_acc,
      apply concat, apply transport4_transport_acc,
      apply concat, apply transport4_transport_acc,
      apply concat, apply transport4_transport_acc,
      apply concat, apply transport4_transport_acc,
      apply transport4_set_reduce,
\end{leancodebr}
The two calls to \leani{apply respect_id₁'} are the use of that fact that each of the
double functors respects identity squares.
The lemmas \leani{twoF_transport_l} and \leani{twoF_transport_r} serve to pull
a transport out of the application of a double functor to a two-cell.
Just as in many proofs, we then end up with a goal that consists of an equation
with the same term on its left and right hand side but with lots of transport
terms on one side.
Since the transports are all on the different faces of a two-cell, we create an
auxiliary definition \leani{transport4} holding transports for each face.
Then we turn the regular transports into instances of this new definitions using
\leani{transport_eq_transport4} and \leani{transport4_transport_acc} so that in the
end we can use the fact that morphisms between two objects form a set to eliminate
the \leani{transport4} term.

\leani{transport4} itself ist defined as follows:
\begin{leancode}
  parameters {A B C D : Type} (P : A → B → C → D → Type)
  definition transport4 {a0 a1 : A} {b0 b1 : B} {c0 c1 : C} {d0 d1 : D}
    (pa : a0 = a1) (pb : b0 = b1) (pc : c0 = c1) (pd : d0 = d1)
    (u : P a0 b0 c0 d0) : P a1 b1 c1 d1 :=
  pd ▹ pc ▹ pb ▹ pa ▹ u
\end{leancode}

After turning the outermost \leani{transport} into a \leani{transport4} we can
accumulate other transport by just using the following lemma:
\begin{leancode}
  definition transport4_transport_acc {E : Type}
    {a0 : A} {b0 : B} {c0 : C} {d0 : D}
    {e0 e1 : E} {f : E → A} {g : E → B} {h : E → C} {i : E → D}
    (pa : f e1 = a0) (pb : g e1 = b0) (pc : h e1 = c0) (pd : i e1 = d0)
    (p : e0 = e1) (u : P (f e0) (g e0) (h e0) (i e0)) :
  transport4 pa pb pc pd (transport (λ (x : E), P (f x) (g x) (h x) (i x)) p u)
  = transport4 (ap f p ⬝ pa) (ap g p ⬝ pb) (ap h p ⬝ pc) (ap i p ⬝ pd) u :=
  by cases pa; cases pb; cases pc; cases pd; cases p; apply idp
\end{leancode}

The final reduction step is done by assuming that all parameter types \leani{A},
\leani{B}, \leani{C}, and \leani{D} are sets:
\begin{leancodebr}
  definition transport4_set_reduce [HA : is_hset A] [HB : is_hset B]
    [HC : is_hset C] [HD : is_hset D]
    {a0 : A} {b0 : B} {c0 : C} {d0 : D}
    {pa : a0 = a0} {pb : b0 = b0} (pc : c0 = c0) (pd : d0 = d0)
    (u : P a0 b0 c0 d0) :
    transport4 pa pb pc pd u = u :=
  begin
    assert Ppa : pa = idp, apply is_hset.elim,
    assert Ppb : pb = idp, apply is_hset.elim,
    assert Ppc : pc = idp, apply is_hset.elim,
    assert Ppd : pd = idp, apply is_hset.elim,
    rewrite [Ppa, Ppb, Ppc, Ppd],
  end
\end{leancodebr}

Finally, we can instantiate the precategory of double groupoids:
\begin{leancodebr}
  universe variables l₁ l₂ l₃
  definition cat_dbl_gpd [reducible] :
    precategory.{(max l₁ l₂ l₃)+1 (max l₁ l₂ l₃)} Dbl_gpd.{l₁ l₂ l₃} :=
  begin
    fapply precategory.mk,
      intros [D, E], apply (dbl_functor D E),
      intros [D, E], apply (is_hset_dbl_functor D E),
      intros [C, D, E, G, F], apply (dbl_functor.compose G F),
      intro D, apply (dbl_functor.id D),
      intros [B, C, D, E, H, G, F], apply (dbl_functor.assoc),
      intros [B, C, F], apply (dbl_functor.id_left),
      intros [B, C, F], apply (dbl_functor.id_right),
  end
\end{leancodebr}

\section{Formalizing Crossed Modules}

The definition of a crossed module involves stating the fact that the given map
$\mu$ is a group homomorphism and that $\phi$ is a groupoid action.
Since the algebra library did not contain any definitions for these algebraic
notions and since their properties are only rarely used, I decided to add them
to the definition of a crossed module as additional fields:
\begin{leancodebr}
structure xmod {P₀ : Type} [P : groupoid P₀] (M : P₀ → Group) :=
  (P₀_hset : is_hset P₀)
  (μ : Π ⦃p : P₀⦄, M p → hom p p)
  (μ_respect_comp : Π ⦃p : P₀⦄ (b a : M p), μ (b * a) = μ b ∘ μ a)
  (μ_respect_id : Π (p : P₀), μ 1 = ID p)
  (φ : Π ⦃p q : P₀⦄, hom p q → M p → M q)
  (φ_respect_id : Π ⦃p : P₀⦄ (x : M p), φ (ID p) x = x)
  (φ_respect_P_comp : Π ⦃p q r : P₀⦄ (b : hom q r) (a : hom p q) (x : M p),
    φ (b ∘ a) x = φ b (φ a x))
  (φ_respect_M_comp : Π ⦃p q : P₀⦄ (a : hom p q) (y x : M p),
    φ a (y * x) = (φ a y) * (φ a x))
  (CM1 : Π ⦃p q : P₀⦄ (a : hom p q) (x : M p), μ (φ a x) = a ∘ (μ x) ∘ a⁻¹)
  (CM2 : Π ⦃p : P₀⦄ (c x : M p), φ (μ c) x = c * (x * c⁻¹ᵍ))
\end{leancodebr}

The fact that $\mu$ respects inverses and $\phi$ respects the neutral element
of the group can then be derived from the fields listed in the definition of
a crossed module:
\begin{leancodebr}
  definition μ_respect_inv ⦃p : P₀⦄ (a : M p) : μ MM a⁻¹ᵍ = (μ MM a)⁻¹ :=
  begin
    assert H : μ MM a⁻¹ᵍ ∘ μ MM a = (μ MM a)⁻¹ ∘ μ MM a,
      exact calc μ MM a⁻¹ᵍ ∘ μ MM a = μ MM (a⁻¹ᵍ * a) : μ_respect_comp
                               ... = μ MM 1 : by rewrite mul_left_inv
                               ... = id : μ_respect_id
                               ... = (μ MM a)⁻¹ ∘ μ MM a : left_inverse,
    apply epi.elim, exact H,
  end

  definition φ_respect_one ⦃p q : P₀⦄ (a : hom p q) : φ MM a 1 = 1 :=
  begin
    assert H : φ MM a 1 * 1 = φ MM a 1 * φ MM a 1,
      exact calc φ MM a 1 * 1 = φ MM a 1 : mul_one
                          ... = φ MM a (1 * 1)  : one_mul
                          ... = φ MM a 1 * φ MM a 1 : φ_respect_M_comp,
    apply eq.inverse, apply (mul_left_cancel H),
  end
\end{leancodebr}

Just like we did for double functors, we define morphisms of crossed modules as
their own structures, with lemmas to state a representation by iterated sigma
and product types, their truncation level and a lemma to build an equality between
two of them:
\begin{leancodebr}
  structure xmod_morphism : Type :=
    (gpd_functor : functor (Groupoid.mk X (gpd X)) (Groupoid.mk Y (gpd Y)))
    (hom_family : Π (p : X), (groups X p) → (groups Y (gpd_functor p)))
    (hom_family_hom : Π (p : X) (x y : groups X p),
      hom_family p (x * y) = hom_family p x * hom_family p y)
    (mu_commute : Π (p : X) (x : groups X p), 
      gpd_functor (μ X x) = μ Y (hom_family p x))
    (phi_commute : Π (p q : X) (a : hom p q) (x : groups X p),
      hom_family q (φ X a x) = φ Y (gpd_functor a) (hom_family p x))

  definition xmod_morphism_sigma_char :
    (Σ (gpd_functor : functor (Groupoid.mk X (gpd X)) (Groupoid.mk Y (gpd Y)))
      (hom_family : Π (p : X), (groups X p) → (groups Y (gpd_functor p))),
      (Π (p : X) (x y : groups X p),
        hom_family p (x * y) = (hom_family p x) * (hom_family p y))
      × (Π (p : X) (x : groups X p),
      to_fun_hom gpd_functor (μ X x) = μ Y (hom_family p x))
      × (Π (p q : X) (a : @hom X _ p q) (x : groups X p),
      hom_family q (φ X a x) = φ Y (gpd_functor a) (hom_family p x)))
        ≃ xmod_morphism := ...

  definition xmod_morphism_hset : is_hset xmod_morphism := ...

  parameters ...
    (p : to_fun_ob gpd_functor1 = to_fun_ob gpd_functor2)
    (q : transport (λ x, Π a b, hom a b → hom (x a) (x b)) p
      (to_fun_hom gpd_functor1) = to_fun_hom gpd_functor2)
    (r : transport (λ x, Π p', (groups X p') → (groups Y (x p'))) p 
      hom_family1 = hom_family2)

  definition xmod_morphism_congr :
    xmod_morphism.mk gpd_functor1 hom_family1 
      hom_family_hom1 mu_commute1 phi_commute1
    = xmod_morphism.mk gpd_functor2 hom_family2
      hom_family_hom2 mu_commute2 phi_commute2 := ...
\end{leancodebr}

This equality lemma can then be uses to prove the identity and associativity of
crossed module morphisms to build the precategory of crossed modules.

\begin{leancodebr}
  definition xmod_morphism_id_left :
    xmod_morphism_comp (xmod_morphism_id Y) f = f :=
  begin
    cases f,
    fapply xmod_morphism_congr,
        apply idp,
      apply idp,
    repeat (apply eq_of_homotopy ; intros),
    apply idp,
  end

  universe variables l₁ l₂ l₃
  definition cat_xmod :
    precategory.{(max l₁ l₂ l₃)+1 (max l₁ l₂ l₃)} Xmod.{l₁ l₂ l₃} :=
  begin
    fapply precategory.mk,
      intros [X, Y], apply (xmod_morphism X Y),
      intros [X, Y], apply xmod_morphism_hset,
      intros [X, Y, Z, g, f], apply (xmod_morphism_comp g f),
      intro X, apply xmod_morphism_id,
      intros [X, Y, Z, W, h, g, f], apply xmod_morphism_assoc,
      intros [X, Y, f], apply xmod_morphism_id_left,
    intros [X, Y, f], apply xmod_morphism_id_right,
  end
\end{leancodebr}



\section{Proving the Equivalence}

Proving the equivalence between the categories $\DGpd$ and $\XMod$ involves the
following steps:
\begin{enumerate}
\item Define $\gamma$ and $\lambda$ on objects -- as functions mapping double
groupoids to crossed modules and vice versa.
\item Define $\gamma$ and $\lambda$ on double functors and crossed module morphisms.
A step that we glossed over when defining the functors in the original topological and
set theoretic setting.
\item Instantiate $\gamma$ and $\lambda$ as actual functors by proving that
they respect identities and composition.
\item Build natural transformations $\gamma\lambda \to \id_{\XMod}$ and
$\lambda\gamma \to \id_{\DGpd}$.
\item Show that these natural transformations are isomorphic by showing that they
map each point in the respective category to an isomorphism in that category.
\end{enumerate} %TODO specify how much of this is completed

We start by implementing $\gamma$ as a function \leani{Dbl_gpd → Xmod}.
This first of all involved building the family of groups in the desired
crossed module.
We create a further structure to hold the objects of each of these groups:
The type of two-cells which have the identity on all but their upper face.
\begin{leancodebr}
  structure folded_sq (a : D₀) :=
    (lid : hom a a)
    (filler : D₂ lid id id id)
\end{leancodebr}

After proving the group axioms for \leani{folded_sq a}, a task which again involves
extensive transport management, we can instantiate the actual group of ``folded
squares'' over a point \leani{a : D₀}.
Here, \leani{l₂} is the the universe level of morphisms in the double groupoid in
the context and \leani{l₃} is the level of two-cells.
\begin{leancodebr}
  protected definition folded_sq_group [instance] (a : D₀) :
    group (folded_sq a) :=
  begin
    fapply group.mk,
      intros [u, v], apply (folded_sq.comp u v),
      apply (folded_sq.is_hset a),
      intros [u, v, w], apply ((folded_sq.assoc u v w)⁻¹),
      apply folded_sq.one,
      intro u, apply (folded_sq.id_left u),
      intro u, apply (folded_sq.id_right u),
      intro u, apply (folded_sq.inv u),
      intro u, apply (folded_sq.left_inverse u),
  end

  protected definition folded_sq_Group [reducible] (a : D₀) :
    Group.{max l₂ l₃} :=
  Group.mk (folded_sq a) (folded_sq_group a)
\end{leancodebr}

The definitions for $\mu$ and $\phi$ themselves are again very straightforward, while
some of the axioms are surprisingly hard to prove, one example being the proof that
$\phi$ respects the group composition, which takes more than 100 lines due to
the massive need to transform \leani{transport} terms.
In the end we can define the bundled crossed module that results from $\gamma$:
\begin{leancodebr}
  protected definition xmod [reducible] : 
    xmod (λ x, gamma.folded_sq_Group G x) :=
  begin
    fapply xmod.mk,
      exact D₀set,
      intros [x, u], apply (gamma.mu G u),
      intros [x, v, u], apply (gamma.mu_respect_comp G v u),
      intro x, apply gamma.mu_respect_id,
      intros [x, y, a, u], apply (gamma.phi G a u),
      intros [x, u], apply (gamma.phi_respect_id G u),
      intros [x, y, z, b, a, u], apply (gamma.phi_respect_P_comp G b a u),
      intros [x, y, a, v, u], apply (gamma.phi_respect_M_comp G a v u),
      intros [x, y, a, u], apply (gamma_CM1 a u),
      intros [x, v, u], apply (gamma_CM2 v u),
  end

  end

  open Dbl_gpd
  protected definition on_objects [reducible] (G : Dbl_gpd) : Xmod :=
  Xmod.mk (λ x, gamma.folded_sq_Group G x) (gamma.xmod G)
\end{leancodebr}

Defining the functor $\gamma$ on morphisms we have to map a double functor to a
morphism of crossed modules.
We first define the morphism on the group family by applying the double functor
to the ``folded squares'' with transports to keep three of their faces the identity.
Proving that this defines a group homomorphism, and proving the further axioms
again makes use of the statements that functors respect composition and identities
and of the helper lemmas to move the resulting transport terms.
\begin{leancodebr}
  context
  parameters {G H : Dbl_gpd} (F : dbl_functor G H)

  protected definition on_morphisms_on_base [reducible]
    (p : gamma.on_objects G) (x : Xmod.groups (gamma.on_objects G) p) :
    Xmod.groups (gamma.on_objects H) (to_fun_ob (dbl_functor.catF F) p) :=
  begin
    cases F with [catF, twoF, F3, F4, F5, F6],
    cases G with [gpdG,sqG,structG,carrierG_hset],
    cases H with [gpdH,sqH,structH,carrierH_hset],
    cases x with [lid,filler],
    fapply folded_sq.mk, apply (to_fun_hom catF lid),
    apply (transport (λ x, sqH _ x id id) (respect_id catF p)),
    apply (transport (λ x, sqH _ _ x id) (respect_id catF p)),
    apply (transport (λ x, sqH _ _ _ x) (respect_id catF p)),
    apply (twoF filler),
  end

  set_option unifier.max_steps 30000
  protected definition on_morphisms_hom_family [reducible]
    (p : Xmod.carrier (gamma.on_objects G)) 
    (x y : Xmod.groups (gamma.on_objects G) p) :
    gamma.on_morphisms_on_base F p (x * y) =
    (gamma.on_morphisms_on_base F p x) * (gamma.on_morphisms_on_base F p y) :=
  begin
    ...
  end
  
  protected definition on_morphisms :
    xmod_morphism (gamma.on_objects G) (gamma.on_objects H) :=
  begin
    ...
  end
\end{leancodebr}

We show the final instantiation of $\gamma$ as a functor using function extensionality
for the proof that $\gamma$ respects composition and identity.
Note that 
\begin{leancodebr}
  protected definition functor :
    functor Cat_dbl_gpd.{l₁ l₂ l₃} Cat_xmod.{(max l₁ l₂) l₂ l₃} :=
  begin
    fapply functor.mk,
      intro G, apply (gamma.on_objects G),
      intros [G, H, F], apply (gamma.on_morphisms F),
      intro G, cases G,
        fapply xmod_morphism_congr, apply idp, apply idp,
        repeat ( apply eq_of_homotopy ; intros), cases x_1, apply idp,
    ...
  end
\end{leancodebr}

Just like for $\gamma$, we create a structure to hold the two-cell of the double
groupoid we create when defining the functor $\lambda$:
\begin{leancode}
  parameters {P₀ : Type} [P : groupoid P₀] {M : P₀ → Group} (MM : xmod M)
  ...
  structure lambda_morphism ⦃a b c d : P₀⦄
    (f : hom a b) (g : hom c d) (h : hom a c) (i : hom b d) :=
  (m : M d) (comm : μ MM m = i ∘ f ∘ h⁻¹ ∘ g⁻¹)
\end{leancode}

Compositions, identity squares, thin squares and axioms that are needed to build
a double groupoid are defined without major difficulties.
In the end we collect all 27 of these definitions to form the double groupoid we
want:
\begin{leancodebr}
  protected definition dbl_gpd [reducible] : dbl_gpd P lambda_morphism :=
  begin
    fapply dbl_gpd.mk,
      intros, apply (lambda_morphism.comp₁ a_1 a_2),
      intros, apply (lambda_morphism.ID₁ f),
      intros, apply lambda_morphism.assoc₁,
      intros, apply lambda_morphism.id_left₁,
      intros, apply lambda_morphism.id_right₁,
      intros, apply lambda_morphism.is_hset,
      intros, apply (lambda_morphism.comp₂ a_1 a_2),
      intros, apply (lambda_morphism.ID₂ f),
      intros, apply lambda_morphism.assoc₂,
      intros, apply lambda_morphism.id_left₂,
      intros, apply lambda_morphism.id_right₂,
      intros, apply lambda_morphism.is_hset,
      intros, apply lambda_morphism.id_comp₁,
      intros, apply lambda_morphism.id_comp₂,
      intros, apply lambda_morphism.zero_unique,
      intros, apply lambda_morphism.interchange,
      intros, apply (lambda_morphism.inv₁ a_1),
      intros, apply lambda_morphism.left_inverse₁,
      intros, apply lambda_morphism.right_inverse₁,
      intros, apply (lambda_morphism.inv₂ a_1),
      intros, apply lambda_morphism.left_inverse₂,
      intros, apply lambda_morphism.right_inverse₂,
      intros, fapply thin_structure.mk,
        intros, apply (lambda_morphism.T f g h i a_1),
        intros, apply lambda_morphism.thin_ID₁,
        intros, apply lambda_morphism.thin_ID₂,
        intros, apply lambda_morphism.thin_comp₁,
        intros, apply lambda_morphism.thin_comp₂,
  end
\end{leancodebr}

After defining $\lambda$ on morphisms we can instantiate it as a functor between
categories with the same restriction to universe levels that we already saw in
the definition of $\gamma$.
\begin{leancodebr}
  protected definition functor :
    functor Cat_xmod.{l₁ l₂ l₃} Cat_dbl_gpd.{(max l₁ l₂) l₂ l₃} :=
  begin
    fapply functor.mk,
      intro X, apply (lambda.on_objects X),
      intros [X, Y, f], apply (lambda.on_morphisms f),
      intro X, cases X,
        fapply dbl_functor.congr', apply idp, apply idp,
        repeat ( apply eq_of_homotopy ; intros), cases x_8,
        fapply lambda_morphism.congr', apply idp,
        apply is_hset.elim,
      intros [X, Y, Z, g, f], cases X, cases Y, cases Z, cases g, cases f,
        fapply dbl_functor.congr', apply idp, apply idp,
        repeat ( apply eq_of_homotopy ; intros), cases x_8,
        fapply lambda_morphism.congr', apply idp,
        apply is_hset.elim,
  end
\end{leancodebr}

\section{Instantiating the Fundamental Double Group\-oid}

The general strategy for the instantiation of the fundamental double group\-oid
of a presented 2-type has been laid out in definition~\ref{def:fundamental-dbl-gpd}.
We start by building the fundamental groupoid of a 1-type $A$, a set $C$ and a
function $\iota : C \to A$ relating them:
\begin{leancodebr}
  definition fundamental_groupoid [reducible] : groupoid C :=
  groupoid.mk
    (λ (a b : C), ι a =  ι b)
    (λ (a b : C), is_trunc_eq nat.zero (ι a) (ι b))
    (λ (a b c : C) (p : ι b = ι c) (q : ι a = ι b), q ⬝ p)
    (λ (a : C), refl (ι a))
    (λ (a b c d : C) (p : ι c = ι d) (q : ι b = ι c) (r : ι a = ι b),
      con.assoc r q p)
    (λ (a b : C) (p : ι a = ι b), con_idp p)
    (λ (a b : C) (p : ι a = ι b), idp_con p)
    (λ ⦃a b : C⦄ (p : ι a = ι b),
      @is_iso.mk C _ a b p (eq.inverse p) (!con.right_inv) (!con.left_inv))
\end{leancodebr}

We then continue to provide all the components for the double groupoid in the ``flat''
setting where we only consider points, equalities and iterated equalities in the
2-type $X$.
This is done by basic manipulation of the given equalities.
The vertical composition, for example, is given by the following calculation:
\begin{leancodebr}
  definition fund_dbl_precat_flat_comp₁ {a₁ b₁ a₂ b₂ a₃ b₃ : X}
    {f₁ : a₁ = b₁} {g₁ : a₂ = b₂} {h₁ : a₁ = a₂} {i₁ : b₁ = b₂}
    {g₂ : a₃ = b₃} {h₂ : a₂ = a₃} {i₂ : b₂ = b₃}
    (v : h₂ ⬝ g₂ = g₁ ⬝ i₂) (u : h₁ ⬝ g₁ = f₁ ⬝ i₁) :
    (h₁ ⬝ h₂) ⬝ g₂ = f₁ ⬝ (i₁ ⬝ i₂) :=
  calc (h₁ ⬝ h₂) ⬝ g₂ = h₁ ⬝ (h₂ ⬝ g₂) : by rewrite con.assoc
                 ... = h₁ ⬝ (g₁ ⬝ i₂) : by rewrite v
                 ... = (h₁ ⬝ g₁) ⬝ i₂ : by rewrite con.assoc'
                 ... = (f₁ ⬝ i₁) ⬝ i₂ : by rewrite u
                 ... = f₁ ⬝ (i₁ ⬝ i₂) : by rewrite con.assoc
\end{leancodebr}

The corresponding axioms are proven by induction over the equalities involved.
Here, one has to carefully choose the order in which to destruct the equalities,
since the iterated equalities can only be destructed if their right hand side
is atomic and since in Lean $p \ct \refl$, but not $\refl \ct p$ is judgmentally
equal to $p$.
The following example shows the proof of the interchange law by destruction in the
order which is illustrated in
figure~\ref{fig:interchange-destruct} by
non-reflexivity equalities shown as paths and iterated equalities as bounded
areas.
\begin{leancodebr}
  variables
    {a₀₀ a₀₁ a₀₂ a₁₀ a₁₁ a₁₂ a₂₀ a₂₁ a₂₂ : X}
    {f₀₀ : a₀₀ = a₀₁} {f₀₁ : a₀₁ = a₀₂} {f₁₀ : a₁₀ = a₁₁} {f₁₁ : a₁₁ = a₁₂}
    {f₂₀ : a₂₀ = a₂₁} {f₂₁ : a₂₁ = a₂₂} {g₀₀ : a₀₀ = a₁₀} {g₀₁ : a₀₁ = a₁₁}
    {g₀₂ : a₀₂ = a₁₂} {g₁₀ : a₁₀ = a₂₀} {g₁₁ : a₁₁ = a₂₁} {g₁₂ : a₁₂ = a₂₂}
    (x : g₁₁ ⬝ f₂₁ = f₁₁ ⬝ g₁₂) (w : g₁₀ ⬝ f₂₀ = f₁₀ ⬝ g₁₁)
    (v : g₀₁ ⬝ f₁₁ = f₀₁ ⬝ g₀₂) (u : g₀₀ ⬝ f₁₀ = f₀₀ ⬝ g₀₁)

  definition fund_dbl_precat_flat_interchange :
    fund_dbl_precat_flat_interchange_vert_horiz x w v u
    = fund_dbl_precat_flat_interchange_horiz_vert x w v u :=
  begin
    revert v, revert f₀₁, revert g₀₂,
    revert u, revert f₀₀, revert g₀₁, revert g₀₀,
    revert x, revert f₁₁, revert g₁₂, revert f₂₁,
    revert w, revert f₁₀, revert g₁₁, revert g₁₀,
    cases f₂₀,
    intro g₁₀, cases g₁₀,
    intro g₁₁, cases g₁₁,
    intro f₁₀, intro w, apply (eq.rec_on w),
    intro f₂₁, cases f₂₁,
    intro g₁₂, cases g₁₂,
    intro f₁₁, intro x, apply (eq.rec_on x),
    intro g₀₀, cases g₀₀,
    intro g₀₁, cases g₀₁,
    intro f₀₀, intro u, apply (eq.rec_on u),
    intro g₀₂, cases g₀₂,
    intro f₀₁, apply (eq.rec_on f₀₁),
    intro v,   apply (eq.rec_on v),
    apply idp,
  end
\end{leancodebr}

\begin{figure} \centering
\begin{tikzpicture}[auto,scale=1,color=black,every path/.append style={thick},
	every loop/.style={}]
\begin{scope}[shift={(0,0)}]
\draw (0,0) -- (0,2) -- (2,2) -- (2,0);
\draw (0,1) -- (2,1);
\draw (1,2) -- (1,0) -- (2,0);
\draw[red,line width=3pt] (0,0) -- (1,0);
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,0) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,2) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,0) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,2) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,0) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,2) {};
\end{scope}
\node at (3,1) {$\overset{\text{\leani{cases f₂₀}}}{\leadsto}$};
\begin{scope}[shift={(4,0)}]
\draw (0,2) -- (2,2); \draw (0,1) -- (2,1); \draw (0,0) -- (2,0);
\draw (0,2) -- (0,1); \draw (1,2) -- (1,1) -- (0,0);
\draw (2,2) -- (2,0);
\draw[red,line width=3pt] (0,1) -- (0,0);
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,0) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,2) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,2) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,0) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,2) {};
\end{scope}
\node at (7,1) {$\overset{\text{\leani{cases g₁₀}}}{\leadsto}$};
\begin{scope}[shift={(8,0)}]
\draw (0,1) edge[bend right,red,line width=3pt] (1,1);
\draw (0,2) -- (2,2); \draw (1,1) -- (2,1);
\draw (0,2) -- (0,1) edge[bend right] (2,0);
\draw (1,2) -- (1,1) edge[bend right] (0,1);
\draw (2,2) -- (2,0);
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,2) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,2) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,0) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,2) {};
\end{scope}
\node at (-1,-2) {$\overset{\text{\leani{cases g₁₁}}}{\leadsto}$};
\begin{scope}[shift={(0,-3)}]
\draw (0,2) -- (2,2); \draw (1,1) -- (2,1);
\draw (1,2) -- (1,1); \draw (2,2) -- (2,0);
\draw (0,2) -- (1,1) edge[in=95,out=130,loop] (1,1) -- (2,0);
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (0,2) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (1,2) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,0) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,1) {};
\node [fill,circle,inner sep=0pt,minimum size=5pt] at (2,2) {};
\end{scope}
\end{tikzpicture}
\caption{Destructing a grid of squares to prove the ``flat'' interchange law.}
\label{fig:interchange-destruct}
\end{figure}

The actual vertical composition is then derived from the ``flat'' version by transporting
twice along the functoriality \leani{ap_con} of path concatenation.
\begin{leancodebr}
  definition fund_dbl_precat_comp₁ ...
    (v : ap ι' h₂ ⬝ ap ι' g₂ = ap ι' g₁ ⬝ ap ι' i₂)
    (u : ap ι' h₁ ⬝ ap ι' g₁ = ap ι' f₁ ⬝ ap ι' i₁) :
      ap ι' (h₁ ⬝ h₂) ⬝ ap ι' g₂ = ap ι' f₁ ⬝ ap ι' (i₁ ⬝ i₂) :=
  ((ap_con ι' i₁ i₂)⁻¹) ▹ ((ap_con ι' h₁ h₂)⁻¹) ▹
  @fund_dbl_precat_flat_comp₁ X A C Xtrunc Atrunc Cset
    (ι' (ι a₁)) (ι' (ι b₁)) (ι' (ι a₂)) (ι' (ι b₂)) (ι' (ι a₃)) (ι' (ι b₃))
    (ap ι' f₁) (ap ι' g₁) (ap ι' h₁) (ap ι' i₁)
    (ap ι' g₂) (ap ι' h₂) (ap ι' i₂) v u
\end{leancodebr}

Then we can prove the axioms for the double groupoid performing step 2 and 3
from the proof of definition~\ref{def:fundamental-dbl-gpd}.
Like in the following example of vertical associativity, we first use the flat
version of the axiom and in the end refer to another auxiliary lemma that allows
for path induction in the type $A$.
\begin{leancodebr}
  definition fund_dbl_precat_assoc₁_aux {a₁ a₂ a₃ a₄ b₁ b₂ b₃ b₄ : A}
    (f₁ : a₁ = b₁) (g₁ : a₂ = b₂) (h₁ : a₁ = a₂) (i₁ : b₁ = b₂)
    (g₂ : a₃ = b₃) (h₂ : a₂ = a₃) (i₂ : b₂ = b₃) (g₃ : a₄ = b₄)
    (h₃ : a₃ = a₄) (i₃ : b₃ = b₄)
    (w : (ap ι' h₃) ⬝ (ap ι' g₃) = (ap ι' g₂) ⬝ (ap ι' i₃))
    (v : (ap ι' h₂) ⬝ (ap ι' g₂) = (ap ι' g₁) ⬝ (ap ι' i₂))
    (u : (ap ι' h₁) ⬝ (ap ι' g₁) = (ap ι' f₁) ⬝ (ap ι' i₁)) :
    (transport (λ x, ((ap ι' h₁) ⬝ x) ⬝ (ap ι' g₃) = _) (ap_con ι' h₂ h₃)
     (transport (λ x, _ =  ((ap ι' f₁) ⬝ ((ap ι' i₁) ⬝ x))) (ap_con ι' i₂ i₃)
      (transport (λ x, (x ⬝ (ap ι' g₃)) = _) (ap_con ι' h₁ (concat h₂ h₃))
       (transport (λ x, _ = (ap ι' f₁) ⬝ x) (ap_con ι' i₁ (concat i₂ i₃))
        (transport (λ x, _ = (ap ι' f₁) ⬝ (ap ι' x)) (con.assoc i₁ i₂ i₃)
         (transport (λ x, (ap ι' x) ⬝ _ = _) (con.assoc h₁ h₂ h₃)
          (transport (λ x, _ =  (ap ι' f₁) ⬝ x) (ap_con ι' (concat i₁ i₂) i₃)⁻¹
           (transport (λ x, x ⬝ (ap ι' g₃) = _) (ap_con ι' (concat h₁ h₂) h₃)⁻¹
            (transport (λ x, _ = _ ⬝ (x ⬝ _)) (ap_con ι' i₁ i₂)⁻¹
             (transport (λ x, (x ⬝ _) ⬝ _ = _) (ap_con ι' h₁ h₂)⁻¹
              (transport (λ x, x ⬝ _ = _)
                (con.assoc (ap ι' h₁) (ap ι' h₂) (ap ι' h₃))⁻¹
                (transport (λ x, _ = _ ⬝ x)
                   (con.assoc (ap ι' i₁) (ap ι' i₂) (ap ι' i₃))⁻¹
                   (fund_dbl_precat_flat_comp₁
                     (fund_dbl_precat_flat_comp₁ w v) u)))))))))))))
     = (fund_dbl_precat_flat_comp₁ (fund_dbl_precat_flat_comp₁ w v) u) :=
  begin
    ...
  end

  definition fund_dbl_precat_assoc₁ :
    (con.assoc i₁ i₂ i₃) ▹ (con.assoc h₁ h₂ h₃) ▹
      (fund_dbl_precat_comp₁ w (fund_dbl_precat_comp₁ v u))
    = fund_dbl_precat_comp₁ (fund_dbl_precat_comp₁ w v) u :=
  begin
    unfold fund_dbl_precat_comp₁,
    apply tr_eq_of_eq_inv_tr, apply tr_eq_of_eq_inv_tr,
    apply inv_tr_eq_of_eq_tr, apply inv_tr_eq_of_eq_tr,
    apply concat, apply fund_dbl_precat_flat_transp1, apply inv_tr_eq_of_eq_tr,
    apply concat, apply fund_dbl_precat_flat_transp2, apply inv_tr_eq_of_eq_tr,
    apply concat, apply fund_dbl_precat_flat_assoc₁', -- Call flat version
    apply eq_tr_of_inv_tr_eq, apply eq_tr_of_inv_tr_eq,
    apply eq_tr_of_inv_tr_eq, apply eq_tr_of_inv_tr_eq,
    apply eq_inv_tr_of_tr_eq, apply eq_inv_tr_of_tr_eq,
    apply eq_inv_tr_of_tr_eq, apply eq_inv_tr_of_tr_eq,
    apply inverse,
    apply concat, apply fund_dbl_precat_flat_transp3, apply inv_tr_eq_of_eq_tr,
    apply concat, apply fund_dbl_precat_flat_transp4, apply inv_tr_eq_of_eq_tr,
    apply inverse, apply fund_dbl_precat_assoc₁_aux, -- Call half-flat lemma
  end
\end{leancodebr}

Thin squares are defined by the following calculation using, again, the functoriality
of $\ap$.
\begin{leancodebr}
  definition fund_dbl_precat_thin {a b c d : C}
    {f : ι a = ι b} {g : ι c = ι d} {h : ι a = ι c} {i : ι b = ι d}
    (comm : h ⬝ g = f ⬝ i) :
    ap ι' h ⬝ ap ι' g = ap ι' f ⬝ ap ι' i :=
  calc ap ι' h ⬝ ap ι' g = ap ι' (h ⬝ g) : ap_con
                    ... = ap ι' (f ⬝ i) : comm
                    ... = ap ι' f ⬝ ap ι' i : ap_con
\end{leancodebr}

Using the same strategy for the axioms for this thin structure as for the axioms
of the weak double groupoid lets us finally conclude by defining the fundamental
double groupoid:
\begin{leancodebr}
  definition fundamental_dbl_precat : dbl_gpd (fundamental_groupoid)
    (λ (a b c d : C) (f : ι a = ι b) (g : ι c = ι d) (h : ι a = ι c) (i : ι b = ι d),
      ap ι' h ⬝ ap ι' g = ap ι' f ⬝ ap ι' i) :=
  begin
    fapply dbl_gpd.mk,
      ...
    fapply thin_structure.mk,
      ...
  end
\end{leancodebr}

\begin{table}[h]
\begin{center}
\begin{tabular}{l|r|r}
\toprule[1pt]
\multicolumn{1}{c}{Theory} 
	& \multicolumn{1}{c}{Line Count} 
	& \multicolumn{1}{c}{Compilation Time in s} \\ 
\midrule[1pt]
\leani{transport4} & 49 & 0.454\\
\leani{dbl_cat.} & & \\
	\hspace{1em}\leani{basic} & & \\
%	\hspace{1em}\leani{category_of} & & \\
	\hspace{1em}\leani{decl} & & \\
%	\hspace{1em}\leani{functor} & & \\	
\leani{dbl_gpd.} & & \\
	\hspace{1em}\leani{basic} & & \\
	\hspace{1em}\leani{category_of} & & \\
	\hspace{1em}\leani{decl} & & \\
	\hspace{1em}\leani{functor} & & \\
	\hspace{1em}\leani{fundamental} & & \\
\leani{equivalence.} & & \\
	\hspace{1em}\leani{equivalence} & & \\
	\hspace{1em}\leani{gamma_functor} & & \\
	\hspace{1em}\leani{gamma} & & \\
	\hspace{1em}\leani{gamma_group} & & \\
	\hspace{1em}\leani{gamma_morphisms} & & \\
	\hspace{1em}\leani{gamma_mu_phi} & & \\
	\hspace{1em}\leani{lambda_functor} & & \\
	\hspace{1em}\leani{lambda} & & \\
	\hspace{1em}\leani{lambda_morphisms} & & \\
\leani{thin_structure.} & & \\
	\hspace{1em}\leani{basic} & & \\
	\hspace{1em}\leani{decl} & & \\
\leani{xmod.} & & \\
	\hspace{1em}\leani{category_of} & & \\
	\hspace{1em}\leani{decl} & & \\
	\hspace{1em}\leani{morphism} & & \\
\bottomrule[1pt]
\end{tabular}
\caption{The theories of my formalization project.} \label{tab:cat-tree}
\end{center}
\end{table}





