My main goal in this thesis project was the formalization and application of
Ronald Brown's structures for non-abelian algebraic topology in the theorem prover
Lean.
Lean, at the point of time when I started working on it, was still in a very early
stage of development and did not lack any automation but also a basic library
for homotopy type theory.
Thus, we will first take a look at the basic language elements and technologies
used in Lean and then describe the strategies, the structure and the pitfalls
we encountered when building up a library for basic homotopy type theory,
for categories in homotopy type theory, and finally for double groupoids and
crossed modules.

\section{The Lean Theorem Prover}

The development of the theorem prover Lean was initiated in 2013 by Leo\-nar\-do
de Moura and Jeremy Avigad.
De Moura had previously been working on the automated theorem prover Z3, the leading
solver for problem sets in the SMT standard.
With Lean, he intends to create a interactive theorem proving system that connects
the strength of solvers like Z3 with the expressiveness and flexibility of
interactive systems like Agda, Coq or Isabelle.
While in the world of automated theorem proving the verification of a statement
results in a yes-or-no answer at best accompanied by a counterexample in the case
that the statement gets refuted, in interactive theorem proving, we are interested
in an actual proof that a statement is correct.
Since in homotopy type theory it is relevant which proof of a theorem we assume
and since proofs of theorems can be part of another definition or theorem,
the proofs in an interactive theorem prover suitable for homotopy type theory
should even be objects in the language itself.
Lean has two modes: One for standard, proof irrelevant mathematics and one for
In the following, I will only explain the features of the HoTT mode.
homtopy type theory.

A first ingredient are \textbf{type universes}.
Instead of using $\UU$, universes in Lean are denoted as \leani{Type.{l}}, where
\leani{l} is the level of the universe.
Of course, \leani{Type.{l}} is an object of \leani{Type.{l+1}}.
But in contrast to homotopy type theory as presented in the HoTT book \cite{hottbook},
type universes in Lean are \emph{non-cumulative}, 
i.e. \leani{A : Type.{l}} does not entail \leani{A : Type.{l+1}}.

Definitions can be \emph{universe polymorphic}, which means that, when no concrete
universe levels are given, Lean will keep the definition as general as possible.
The instantiation of a definition \leani{A} at a universe \leani{l} can be received
manually by writing \leani{A.{l}}.
To have manual control over the coherence of universe levels of definitions in
a certain scope, variable universe levels can be declared using the command
\leani{universe variable}. The following snippet shows universe polymorphism
and the use of universe variables:
\begin{leancode}
check Type -- Prints Type.{l_1} : Type.{l_1+1}

universe variable l
check Type.{l} -- Prints Type.{l} : Type.{l+1}
\end{leancode}

The only built-in type formers are (dependent and non-dependent) function types,
structures, and inductive datatypes.

The \textbf{type of functions} between types \leani{A} and \leani{B} is written
as \leani{A → B}.
\leani{A} and \leani{B} do not have to lie in the same universe to form this type
and the universe level of the function type is the maximum of the level of domain
and codomain type.
Lambda abstraction and function application can be written like known from e.g.
Haskell.
$\beta$ reduction is done for each output, $\eta$ conversion is applied when necessary
in the unification process.
\begin{leancode}
variables (A B : Type) (a : A) (b : B) (f : A → B)

check A → B -- Prints A → B : Type.{max l_1 l_2}
check (λ (x : A), b) -- Prints λ (x : A), b : A → B
check (f a) -- Prints f a : B
check (λ x, f x) a -- Prints f a : B
\end{leancode}

An important special case of non-dependent function types are type families
of the form \leani{A → Type}. For every \leani{P : A → Type} we can form the
\textbf{$\Pi$-type} \leani{Π (x : A), P x} over \leani{P}.
Actually, non-dependent function types are just treated as the special case of
dependent functions where \leani{P} is constant.
The $\Pi$-type \leani{Π (x : A), B} for \leani{A B : Type} is automatically reduced
to \leani{A → B}.
\begin{leancode}
variables (A B : Type) (P : A → Type) (Q : Π (x : A) , P x → Type)
variables (p : Π (x : A), P x) (a : A)

check p a -- Prints p a : P a
check Q a -- Prints Q a : P a → Type
check (λ (x : A), Q x (p x)) -- Prints λ (x : A), Q x (p x) : A → Type
\end{leancode}

Lean furthermore allows the definition of inductive types and inductive
families.
To construct an \textbf{inductive type}, one must give a list of parameters the type should
depend on and a list of constructors.
This makes the definition of important types like the natrual numbers or the identity
type possible.
The dependent recursor for inductive types is generated automatically by the kernel:
\begin{leancode}
inductive nat : Type :=
  zero : nat,
  succ : nat → nat
check nat.succ nat.zero) -- Prints nat.succ nat.zero : nat
check @nat.rec_on -- Prints Π {C : nat → Type} (n : nat), C nat.zero →
                  --        (Π (a : nat), C a → C (nat.succ a)) → C n
\end{leancode}

\begin{leancode}
inductive eq (A : Type) (a : A) : A → Type :=
  refl : eq A a a
variables (A : Type) (a : A)
check @eq.refl A a -- Prints eq.refl a : eq A a a
check @eq.rec_on A a -- Prints Π {C : Π (a_1 : A), eq A a a_1 → Type}
                     --        {a_1 : A} (n : eq A a a_1),
                     --        C a (eq.refl a) → C a_1 n
\end{leancode}

We can not only define single inductive types but also \textbf{families of
inductive types}, which we can define by recursion on the index of the family:
\begin{leancode}
open nat

inductive vec (A : Type) : ℕ → Type :=
  nil : vec A 0,
  cons : Π (n : ℕ), A → vec A n → vec A (n+1)

open vec
variables (A : Type) (a : A)
check @vec.rec_on A -- Prints Π {C : Π (a : ℕ), vec A a → Type} {a : ℕ} 
                    --        (n : vec A a), C 0 (nil A) →
                    --        (Π (n : ℕ) (a : A) (a_1 : vec A n), C n a_1
                    --          → C (n + 1) (cons n a a_1)) →
                    --        C a n
check cons 0 a (nil A) -- Prints cons 0 a (nil A) : vec A (0+1)
\end{leancode}

A widely used subclass of inductive types are \textbf{structures}, also often
referred to as "records".
Structures are inductive types which are non-recursive and only have one constructor.
That means that they are equivalent to iterated sigma types but, among other advantages,
have named projections.
Structures provide a basic inheritance mechanism as they can extend arbitrarily
many other structures with \textbf{coercions} being added for each parent structure:
\begin{leancode}
structure graph (V : Type₀) :=
  (E : V → V → Type₀)

structure refl_graph (V : Type₀) extends graph V :=
  (refl : Π (v : V), E v v)

structure trans_graph (V : Type₀) extends graph V :=
  (trans : Π (u v w : V), E u v → E v w → E u w)

structure refl_trans_graph (V : Type₀) extends refl_graph V, trans_graph V

variables (V : Type₀)
check graph.E (refl_graph.mk (λ (a b : V), a = b) eq.refl)
/- Prints
  graph.E (refl_graph.to_graph 
    (refl_graph.mk (λ (a b : V), a = b) eq.refl)) :
    V → V → Type₀ -/
\end{leancode}

As one can already seen in these small examples, writing out the full names and
the complete list of parameters for each call of a defined function can be very
tedious.
Lean implements some features that allow its users to make theory files more
succinct and readable by leaving out information that can be inferred automatically
by Lean.

One feature that allows more brevity are \textbf{implicit arguments}.
To mark an argument to a definition to be automatically inferred by Lean,
the user can mark it with curly brackets instead of round brackets when listing
it in the signatur of the definition.
Of course the missing arguments have to be such that they can be uniquely determined
by the unification process, otherwise the unifier will return an appropriate error
message whenever the definition is used.
The user can make all arguments in a function call implicit by prepending a
\leani{!} to the definition name.
The opposite, making all arguments explicit, can be achieved by prepending the
symbol \leani{@}.
By default, implicit arguments are \emph{maximally inserted}.
That means, that any expression of a $\Pi$-type with its first argument implicit
is not interpreted as is but further applied by inference of that argument.
By using double curly brackets \leani{⦃...⦄} instead of single curly brackts,
the user can change this behaviour to be more passive by only inferring the argument
if it precedes a explicitely stated one:
\begin{leancode}
definition inverse {P : Type} {x y : P} (p : x = y) : y = x :=
  eq.rec (eq.refl x) p
check inverse -- Prints inverse : ?x = ?y → ?y = ?x

definition id ⦃A : Type⦄ (a : A) := A
check id -- Prints id : Π ⦃A : Type⦄, A → Type

variables {A : Type} (a : A)
check id a -- Prints id a : Type
\end{leancode}

Another useful feature to organize formalizations and shorten code is the interaction
between namespaces and overloading.
In general there are no restrictions on \textbf{overloading} theorem names but each additional
overload with the same name makes it harder for the elaborator to figure out which
one it needs to use in a certain case.
To make overloads more organized, definitions can be put in hierarchical \textbf{namespaces}
which can be opened selectively.
Opening a namespace makes each theorem in that namespace available to be called
with its name as stated in the definition inside that namespace, as opposed to
giving its absolute name.
Definitions can be marked as \leani{protected} to prevent their names from being
pruned and as \leani{private} to exclude them from exporting completely.
Furthermore, opening namespaces enables the use of \textbf{notations} defined
in that namespace.
\begin{leancode}
open nat

namespace exponentiation
  definition squared (a : ℕ) := a * a
  notation a`²`:100 := squared a
end exponentiation

open exponentiation
check squared -- Prints squared : ℕ → ℕ
eval 4² -- Prints 16
\end{leancode}
Often, consecutive definitions share a lot of arguments.
To avoid the need to repeat those arguments for each definition, Lean provides
\textbf{sections} and \textbf{contexts} which serve as scopes for common variables.
Variables declared in sections is persistent while variables in contexts will not
be accessible after closing the scope.
Besides \leani{variables}, there are \leani{parameters} which will not be generalized
until the context or section is closed.

One last feature helping to create short and readable files are \textbf{type
classes}.
After marking an inductive type or a family of inductive type as \leani{[class]},
we can declare definitions that are objects of this type as \leani{[instance]}.
Then, we can use a fourth mode of argument implicitness (besides \leani{(...)},
\leani{{...}}, and \leani{⦃...⦄}) denoted by \leani{[...]} to tell Lean that it
should infer this argument by filling in one of the instances of the required
type.
The selection of one out of several fitting instances can be influenced by assigning
priorities to the instance declarations.
Type classes are a tool for achieving \emph{canonicity}.
One important application is the fixation of instances of algebraic structures on
certain types or their closure under certain type formers, for example the structure
of an abelian group on the type of integers or the direct product of groups as
a canonical group structure on a product type.
By marking theorems as instances which themselves have type class parameters,
we can put quite some automation load on the type class resolution.
In the following example we give a canonical graph structure to the natural numbers
and prove that the resulting graph is serial:
\begin{leancode}
open nat eq sigma.ops

structure graph [class] (V : Type₀) := (E : V → V → Type₀)

definition nat_graph [instance] : graph nat := graph.mk (λ x y, y = x+1)

definition is_serial (V : Type₀) [G : graph V] := Π x, Σ y, graph.E x y

definition nat_serial : is_serial nat := λ x, ⟨x+1, idp⟩
\end{leancode}

In all previous examples, we stated all definitions and proofs by giving the
whole term at once.
In some theorem provers like Agda \cite{agda}, this method, support by the presence
of \texttt{let} and \texttt{where} terms, is used for all proofs while others,
like Coq \cite{coq}, rely on the use of \textbf{tactics}.
Tactics are sequentially executed commands that transform a given proof goal into
an easier to solve set of goals.
Lean allows both approaches to be used purely or even using tactics for subterms
of a declarative proofs.
The user can switch to the tactic mode with \leani{begin ... end}.
As soon as the tactic block has been successfully filled in with tactic calls,
the full proof term gets elaborated, type checked, and is from there on in\-
distinguishable from a directly entered proof.

The tactics in Lean are still under development and will be extended in near
future.
Thus, I will only give a list of the tactics that were used at the time of
writing the formalizations in this thesis:
\begin{itemize}
\item The tactic \leani{exact} takes a proof term as argument which it uses
to solve the first subgoal completely.
If Lean fails to infer every argument that is not stated explicitly or if Lean
cannot unify the expression with the goal, the tactic fails.
\item If one wants to treat the current first subgoal with an arbitrary theorem
without already solving it completely, one can use \leani{apply}.
It unifies the theorem with the subgoal but opposed to failing it creates a new
subgoal for each undetermined or unresolved argument of the state theorem.
The new subgoals are added from the last argument to the first.
Arguments, that can be inferred from later arguments will not be converted to a
new subgoal.
\item Sometimes, one wants to prove premises or give arguments in the order
they appear in the statement of the applied theorem.
This can be achieved by using \leani{fapply}.
In general, it results in more subgoals than \leani{apply}.
\item Using \leani{assert} lets the user proof a named statement which is added
to the context for the rest of the main proof.
\item If the current subgoal is a (dependent or non-dependent) function, \leani{intro}
adds the variable, which the goal quantifies over, to the context and applies it
to the goal. \leani{intros} does the same iteratively for multiple variables.
\leani{revert} does the opposite: It selects a variable in the context and
replaces the goal by its quantification over this variable.
A similar job is done by \leani{generalize}.
It takes an arbitrary term as argument and quantifies the goal over a variable
that term's type, replacing all its occurrences by the quantifying variable.
\item The tactic \leani{clear} is used to delete unused variables from the context.
This is especially useful to speed up type class resolution and to shorten the output
of the current context.
\item \leani{cases} destructs a given context variable of an inductive type 
using the automatically generated \leani{cases_on} theorem.
Optionally, a list of the desired names for the newly generated variables can be
appended.
\item Often, a goal is not expressed in its simplest form.
\leani{esimp} simplifies the goal after unfolding a given list of definitions.
\item The \leani{rewrite} tactic takes a theorem yielding a (propositional)
equality and replaces occurrences of the equation's left hand side by the 
right hand side.
The user can specify in what pattern or how often the replacement should be made.
\item By using \leani{rotate}, one can rotate the list of subgoals by a given
number of steps.
This is useful when the user wants to postpone the goal at the first position
to be solved at the end of the proof.
\item Tactics can furthermore be concatenated to a single tactic using the and-then
composition \leani{( tactic1 ; tactic2 )}.
The resulting tactic fails if either one of the tactics fail. \\
The notation \leani{[ tactic1 | tactic2 ]} specifies an or-else branching of tactics:
Lean first tries to apply the first tactic and if it fails the second one.
The composed tactic only fails if both of them fail.
\item Adding to this composition of tactics one can use the \leani{repeat}
tactic as a loop command.
The tactic given to \leani{repeat} as an argument is applied over and over until
it fails.
So, by giving it an or-else composition of tactics Lean tries each one from a set
of tactics until none of them is applicable.
\item Curly brackets can be used to create a scope which only aims to solve the
first subgoal.
If the subgoal is not solved within that scope, an error will appear at the line
of the closing bracket. %TODO make this sound better
\end{itemize} %TODO give some examples maybe?

\section{Basic Homotopy Type Theory in Lean}

Lean's homotopy type theory library is split into two parts:
The content of a folder \texttt{init} is imported by default and provides
theories for built-in features like \leani{rewrite}.
Besides this folder there are other theories which are not required at startup
and which the user can import manually.
They include characterizations of path spaces and basic theories about common
algebraic structures which will be presented in the next chapter.
The extension and maintenance of the library is an ongoing joint effort by
Jeremy Avigad, Floris van Doorn, Leonardo de Moura and me.

One important definition in the initialization folder is, of course, the one of
\textbf{propositional equality}.
As mentioned above, equality is just a very basic example for an inductive type.
Here, we define equality as inductive family of types \leani{eq} and with \leani{idp}
give an alternative name for $\refl$ with the base being implicit:
\begin{leancode}
universe variable l
inductive eq.{l} {A : Type.{l}} (a : A) : A → Type.{l} :=
refl : eq a a

definition idp {a : A} := refl a
\end{leancode}
Concatenation and inversion of paths are defined using path induction and abbreviated
with the obvious notation:
\begin{leancode}
definition concat (p : x = y) (q : y = z) : x = z := eq.rec (λu, u) q p
definition inverse (p : x = y) : y = x := eq.rec (refl x) p

notation p₁ ⬝ p₂ := concat p₁ p₂
notation p ⁻¹ := inverse p
\end{leancode}
By this definition, $p \ct \refl$ is definitionally equal to $p$ while $\refl \ct p = p$
can only be proved by induction on $p$.
A lot of basic calculations in the path groupoid can also be proved by just using
path induction. Her is one example for this kind of lemma:
\begin{leancode}
definition eq_of_idp_eq_inv_con (p q : x = y) : idp = p⁻¹ ⬝ q → p = q :=
eq.rec_on p (take q h, h ⬝ (idp_con _)) q
\end{leancode}
Two other basic definitions for paths are the one of transports and $\ap$.
\begin{leancode}
definition transport [reducible] (P : A → Type) {x y : A}
  (p : x = y) (u : P x) : P y :=
eq.rec_on p u

definition ap ⦃A B : Type⦄ (f : A → B) {x y:A} (p : x = y) : f x = f y :=
eq.rec_on p idp
\end{leancode}

To be able to express univalence it is important to have a useful definition of
the \textbf{equivalence of types}.
We use structures to express the type of proofs $\isequiv(f)$ that a function $f$
is a half-adjoint equivalence as well as for the type of equivalences between
two given types (\textasciitilde \, denotes the type of homotopies between two functions) :
\begin{leancode}
structure is_equiv [class] {A B : Type} (f : A → B) :=
  (inv : B → A)
  (retr : (f ∘ inv) ∼ id)
  (sect : (inv ∘ f) ∼ id)
  (adj : Π x, retr (f x) = ap f (sect x))

structure equiv (A B : Type) :=
  (to_fun : A → B)
  (to_is_equiv : is_equiv to_fun)
\end{leancode}
In practice, we will almost never use the generated constructor of \leani{is_equiv}
but instead we proved an alternative constructor \leani{adjointify} which does
not require the adjointness proof \leani{adj}.

\textbf{Univalence} than states that the function \leani{equiv_of_eq}, which lets
us gain an equivalence from an equality between two types in the same universe,
is itself an equivalence.
We mark the axiom as instance so that, whenever there is mention of \leani{equiv_of_eq},
we have access to its inverse.
This inverse \leani{ua} is also the only common form univalence appears elsewhere.
\begin{leancodebr}
section
  universe variable l
  variables {A B : Type.{l}}

  definition is_equiv_tr_of_eq (H : A = B) :
    is_equiv (transport (λ X, X) H) :=
  @is_equiv_tr Type (λX, X) A B H

  definition equiv_of_eq (H : A = B) : A ≃ B :=
  equiv.mk _ (is_equiv_tr_of_eq H)
end

axiom univalence (A B : Type) : is_equiv (@equiv_of_eq A B)
attribute univalence [instance]

definition ua {A B : Type} : A ≃ B → A = B := (@equiv_of_eq A B)⁻¹
\end{leancodebr}

We use the univalence axiom to prove \textbf{function extensionality} first of
non-dependent and then of dependent functions.
We first define three varieties of function extensionality and then proof that
the desired definition of function extensionality (stating that the function
\leani{apD10} which returns for each equality between functions a homotopy
between those functions) follows from the other ones.
This approach has been ported from the Coq HoTT library
~\footnote{\url{https://github.com/HoTT/HoTT}}.
\begin{leancode}
definition funext.{l k} := 
Π ⦃A : Type.{l}⦄ {P : A → Type.{k}} (f g : Π x, P x),
  is_equiv (@apD10 A P f g)

-- Naive funext is the assertion that pointwise equal functions are equal.
definition naive_funext :=
Π ⦃A : Type⦄ {P : A → Type} (f g : Πx, P x), (f ∼ g) → f = g

-- Weak funext says that a product of contractible types is contractible.
definition weak_funext :=
Π ⦃A : Type⦄ (P : A → Type) [H: Πx, is_contr (P x)], is_contr (Πx, P x)
\end{leancode}

\textbf{Truncation levels} are implemented by first creating a version of the natural
numbers that ``start at -2'' together with a coercion from the actual type
\leani{nat}, then defining internal versions of contractability and truncatedness,
and at eventually defining a type class structure holding a proof of the internal
truncatedness:
\begin{leancodebr}
inductive trunc_index : Type₁ :=
  minus_two : trunc_index,
  succ : trunc_index → trunc_index
...
structure contr_internal (A : Type) :=
  (center : A)
  (contr : Π(a : A), center = a)

definition is_trunc_internal (n : trunc_index) : Type → Type :=
trunc_index.rec_on n (λA, contr_internal A)
  (λn trunc_n A, (Π(x y : A), trunc_n (x = y)))
...
structure is_trunc [class] (n : trunc_index) (A : Type) :=
  (to_internal : is_trunc_internal n A)

abbreviation is_contr := is_trunc -2
abbreviation is_hprop := is_trunc -1
abbreviation is_hset  := is_trunc 0
\end{leancodebr}
By this trick we can then define contractability as the special case of being 
truncated at level -2.
This makes it a lot easier to write theorems that hold for all levels of
truncatedness.
The type class \leani{is_trunc} has many instances that, by calling type class
resolution themselves, automatize the process of finding the truncation level
of a given iterated $\Sigma$-type or $\Pi$-type (proofs and surrounding
context omitted, definitions gathered from multiple files):
\begin{leancodebr}
--Equalities in contractible types are contractible.
definition is_contr_eq {A : Type} [H : is_contr A] (x y : A) :
  is_contr (x = y) := ...

-- n-types are also (n+1)-types.
definition is_trunc_succ [instance] [priority 100] 
  (A : Type) (n : trunc_index) [H : is_trunc n A] : is_trunc (n.+1) A := ...

-- The unit type is contractible.
definition is_contr_unit [instance] : is_contr unit := ...

-- The empty type is a mere proposition.
definition is_hprop_empty [instance] : is_hprop empty := ...

-- A Sigma type is n-truncated if the type of every possible projections is.
definition is_trunc_sigma [instance] (B : A → Type) (n : trunc_index)
  [HA : is_trunc n A] [HB : Πa, is_trunc n (B a)] :
  is_trunc n (Σ a, B a) := ...
  
-- Any dependent product of n-types is an n-type.
definition is_trunc_pi [instance] (B : A → Type) (n : trunc_index)
  [H : Πa, is_trunc n (B a)] : is_trunc n (Πa, B a) := ...
  
-- Being an equivalence is a mere proposition.
theorem is_hprop_is_equiv [instance] : is_hprop (is_equiv f) := ...
\end{leancodebr}

An important part of the library besides the initialization files consists of
theorems characterizing paths between instances of different type formers.
One example for such a characterization is that a path between two dependent
pairs can be built out of paths between their projections:
\begin{leancode}
definition dpair_eq_dpair (p : a = a') (q : p ▹ b = b') : ⟨a, b⟩ = ⟨a', b'⟩ :=
by cases p; cases q; apply idp

definition sigma_eq (p : u.1 = v.1) (q : p ▹ u.2 = v.2) : u = v :=
by cases u; cases v; apply (dpair_eq_dpair p q)
\end{leancode}

\begin{table}[p]
\begin{center}
\begin{tabular}{l|r|r}
\toprule[1pt]
\multicolumn{1}{c}{Theory} 
	& \multicolumn{1}{c}{Line Count} 
	& \multicolumn{1}{c}{Compilation Time in s} \\ 
\midrule[1pt]
\leani{init.} & & \\
	\hspace{1em}\leani{bool} & 28 & 0.043\\
	\hspace{1em}\leani{datatypes} & 90 & 0.044\\
	\hspace{1em}\leani{default} & 15 & 0.421\\
	\hspace{1em}\leani{equiv} & 276 & 1.082\\
	\hspace{1em}\leani{function} & 61 & 0.042\\
	\hspace{1em}\leani{hedberg} & 47 & 0.399\\
	\hspace{1em}\leani{logic} & 359 & 0.173\\
	\hspace{1em}\leani{nat} & 345 & 0.565\\
	\hspace{1em}\leani{num} & 135 & 0.073\\
	\hspace{1em}\leani{path} & 648 & 2.683\\
	\hspace{1em}\leani{priority} & 12 & 0.036\\
	\hspace{1em}\leani{relation} & 43 & 0.072\\
	\hspace{1em}\leani{reserved_notation} & 103 & 0.035\\
	\hspace{1em}\leani{tactic} & 106 & 0.067\\
	\hspace{1em}\leani{trunc} & 262 & 0.821\\
	\hspace{1em}\leani{util} & 18 & 0.324\\
	\hspace{1em}\leani{wf} & 162 & 0.394\\
	\hspace{1em}\leani{axioms.} & & \\
		\hspace{2em}\leani{funext_of_ua} & 162 & 0.674\\
		\hspace{2em}\leani{funext_varieties} & 111 & 0.483\\
		\hspace{2em}\leani{ua} & 51 & 0.302\\
	\hspace{1em}\leani{types.} & & \\
		\hspace{2em}\leani{empty} & 23 & 0.055\\
		\hspace{2em}\leani{prod} & 99 & 0.225\\
		\hspace{2em}\leani{sigma} & 26 & 0.058\\
		\hspace{2em}\leani{sum} & 19 & 0.036\\ %TODO repeat this without -t 0
\bottomrule[1pt]
\end{tabular}
\caption{Theories imported in Lean's initial startup.} \label{tab:init-tree}
\end{center}
\end{table} 

\begin{table}[p]
\begin{center}
\begin{tabular}{l|r|r}
\toprule[1pt]
\multicolumn{1}{c}{Theory} 
	& \multicolumn{1}{c}{Line Count} 
	& \multicolumn{1}{c}{Compilation Time in s} \\ 
\midrule[1pt]
\leani{arity} & 188 & 0.797\\
\leani{algebra.} & & \\
	\hspace{1em}\leani{binary} & 74 & 0.156\\
	\hspace{1em}\leani{group} & 570 & 1.317\\
	\hspace{1em}\leani{relation} & 122 & 0.330\\
\leani{types.} & & \\
	\hspace{1em}\leani{arrow} & 49 & 0.143\\
	\hspace{1em}\leani{eq} & 271 & 0.556\\
	\hspace{1em}\leani{equiv} & 98 & 0.652\\
	\hspace{1em}\leani{fiber} & 51 & 0.199\\
	\hspace{1em}\leani{pi} & 198 & 1.177\\
	\hspace{1em}\leani{pointed} & 40 & 0.121\\
	\hspace{1em}\leani{prod} & 48 & 0.144\\
	\hspace{1em}\leani{sigma} & 397 & 1.599\\
	\hspace{1em}\leani{trunc} & 140 & 0.371\\
	\hspace{1em}\leani{W} & 157 & 0.128\\
\bottomrule[1pt]
\end{tabular}
\caption{Theories in Lean's standard library for its homotopy type theory mode.
(Category theory excluded.)} \label{tab:hottlib-tree}
\end{center}
\end{table}

Figure \ref{tab:init-tree} and \ref{tab:hottlib-tree} show the line count and
the compilation time for each theory in Lean's HoTT library excluding the library
for category theory.

\section{Category Theory in Lean}

Our library of basic category theoretical definitions and theorems, which is
still work in progress, also aims to mimic the structure of Coq's HoTT implementation
of categories while being more succinct than it.
We use structures and type classes for most definitions of algebraic structures
and their closure properties.

The central structure our formalization revolves around is, of course, the one
of a precategory:
\begin{leancode}
structure precategory [class] (ob : Type) : Type :=
  (hom : ob → ob → Type)
  (homH : Π(a b : ob), is_hset (hom a b))
  (comp : Π⦃a b c : ob⦄, hom b c → hom a b → hom a c)
  (ID : Π (a : ob), hom a a)
  (assoc : Π ⦃a b c d : ob⦄ (h : hom c d) (g : hom b c) (f : hom a b),
     comp h (comp g f) = comp (comp h g) f)
  (id_left : Π ⦃a b : ob⦄ (f : hom a b), comp !ID f = f)
  (id_right : Π ⦃a b : ob⦄ (f : hom a b), comp f !ID = f)
\end{leancode}

Since usually the domain of an identity morphism is determined by the context
we will most often use a shorter notation for the identity:
\begin{leancode}
definition id [reducible] := ID a
\end{leancode}

We introduce a shortcut for the type class instance postulating that each equality
of morphism is a mere proposition.
We use this instance, for example, to prove that it is sufficient to give equalities
between the morphism types, composition and identities of two precategories on the
same type of objects, to show that these precategories are equal:

\begin{leancode}
definition is_hprop_eq_hom [instance] : is_hprop (f = f') := !is_trunc_eq

definition precategory_eq_mk' (ob : Type) (C D : precategory ob)
  (p : @hom ob C = @hom ob D)
  (q : transport (λ x, Πa b c, x b c → x a b → x a c) p
    (@comp ob C) = @comp ob D)
  (r : transport (λ x, Πa, x a a) p (@ID ob C) = @ID ob D) : C = D :=
begin
  cases C, cases D,
  apply precategory_eq_mk, apply q, apply r,
end
\end{leancode}

We also define a \emph{bundled} version of a category, as the structure containing
the object type and a precategory on it as fields:
\begin{leancode}
structure Precategory : Type :=
  (carrier : Type)
  (struct : precategory carrier)
\end{leancode}

Using typeclasses for split monos, split epis and isomorphisms enables us to access
the left, right or both-sided inverse filling in the precategory structure as
well as the invertability witness by type class instance resolution:
\begin{leancode}
structure split_mono [class]
    {ob : Type} [C : precategory ob] {a b : ob} (f : a ⟶ b) :=
  {retraction_of : b ⟶ a}
  (retraction_comp : retraction_of ∘ f = id)

structure split_epi [class]
    {ob : Type} [C : precategory ob] {a b : ob} (f : a ⟶ b) :=
  {section_of : b ⟶ a}
  (comp_section : f ∘ section_of = id)

structure is_iso [class]
    {ob : Type} [C : precategory ob] {a b : ob} (f : a ⟶ b) :=
  {inverse : b ⟶ a}
  (left_inverse  : inverse ∘ f = id)
  (right_inverse : f ∘ inverse = id)
\end{leancode}

On purpose, we weaken some theorems to accepting arbitrary witnesses for mere propositions
that would actually be derivable from the other witnesses.
By this, we can have class instance resolution fill in a witness selected by the
highest instance priority.
In the following example, even if there is a theorem obtaining \leani{is_iso (f⁻¹)}
from \leani{is_iso f}, we might want the witness to be the axiom that in a groupoid
all morphisms are isomorphisms.
This spares us the need of transporting the statement to the right witness each
time we use it:
\begin{leancode}
definition inverse_involutive (f : a ⟶ b) [H : is_iso f] [H : is_iso (f⁻¹)]
  : (f⁻¹)⁻¹ = f :=
inverse_eq_right !left_inverse

definition id_inverse (a : ob) [H : is_iso (ID a)] : (ID a)⁻¹ = id :=
inverse_eq_left !id_comp

definition comp_inverse [Hp : is_iso p] [Hpq : is_iso (q ∘ p)] :
  (q ∘ p)⁻¹ʰ = p⁻¹ʰ ∘ q⁻¹ʰ :=
inverse_eq_left (show (p⁻¹ʰ ∘ q⁻¹ʰ) ∘ q ∘ p = id, from
  by rewrite [-assoc, inverse_comp_cancel_left, left_inverse])
\end{leancode}

Not only the the property of a morphism $f$ being an isomorphism but also the type
of morphisms between two given objects is encapsulated in a structure that heavily
helps to fill in proofs automatically. %TODO more precise
\begin{leancode}
structure iso (a b : ob) :=
  (to_hom : hom a b)
  [struct : is_iso to_hom]

infix `≅`:50 := iso.iso
attribute iso.struct [instance] [priority 400]

-- The type of isomorphisms between two objects is a set.
definition is_hset_iso [instance] : is_hset (a ≅ b) :=
begin
  apply is_trunc_is_equiv_closed,
    apply (equiv.to_is_equiv (!iso.sigma_char)),
end
\end{leancode}

Another definition we want to generalize to the two dimensional case is the one
of a functor.
We introduce functors as a structure and add coercions to its function on objects
and to its function on morphisms.
By doing so, we will be able to write \leani{F a} for the image of an object \leani{a}
and \leani{F f} for the image of a morphism \leani{f} under a functor \leani{F}.
\begin{leancode}
structure functor (C D : Precategory) : Type :=
  (to_fun_ob : C → D)
  (to_fun_hom : Π ⦃a b : C⦄, hom a b → hom (to_fun_ob a) (to_fun_ob b))
  (respect_id : Π (a : C), to_fun_hom (ID a) = ID (to_fun_ob a))
  (respect_comp : Π {a b c : C} (g : hom b c) (f : hom a b),
    to_fun_hom (g ∘ f) = to_fun_hom g ∘ to_fun_hom f)

infixl `⇒`:25 := functor
attribute to_fun_ob [coercion]
attribute to_fun_hom [coercion]
\end{leancode}

One example where these coercions are used is the definition of the composition
of functors:
\begin{leancode}
definition compose [reducible] (G : functor D E) (F : functor C D) :
  functor C E :=
functor.mk
  (λ x, G (F x))
  (λ a b f, G (F f))
  (λ a, calc
    G (F (ID a)) = G (ID (F a)) : by rewrite respect_id
             ... = ID (G (F a)) : by rewrite respect_id)
  (λ a b c g f, calc
    G (F (g ∘ f)) = G (F g ∘ F f)     : by rewrite respect_comp
              ... = G (F g) ∘ G (F f) : by rewrite respect_comp)
\end{leancode}

Like mentioned above, it is useful to still have a representation of a structure
as an iterated product and $\Sigma$-type.
With this characterization we can formalize lemma \ref{thm:functors-hset} easily:
\begin{leancode}
protected definition sigma_char :
  (Σ (to_fun_ob : C → D)
  (to_fun_hom : Π ⦃a b : C⦄, hom a b → hom (to_fun_ob a) (to_fun_ob b)),
  (Π (a : C), to_fun_hom (ID a) = ID (to_fun_ob a)) ×
  (Π {a b c : C} (g : hom b c) (f : hom a b),
    to_fun_hom (g ∘ f) = to_fun_hom g ∘ to_fun_hom f)) ≃ (functor C D) :=
...

set_option apply.class_instance false
protected definition is_hset_functor [HD : is_hset D] : is_hset (C ⇒ D) :=
begin
  apply is_trunc_is_equiv_closed, apply equiv.to_is_equiv,
    apply sigma_char,
  apply is_trunc_sigma, apply is_trunc_pi, intros, exact HD, intro F,
  apply is_trunc_sigma, apply is_trunc_pi, intro a,
    {apply is_trunc_pi, intro b,
     apply is_trunc_pi, intro c, apply !homH},
  intro H, apply is_trunc_prod,
    {apply is_trunc_pi, intro a,
     apply is_trunc_eq, apply is_trunc_succ, apply !homH},
    {repeat (apply is_trunc_pi; intros),
     apply is_trunc_eq, apply is_trunc_succ, apply !homH},
end
\end{leancode}

This enables us to implement the precategory of strict precategory (compare
corollary \ref{thm:precat-strict-precat}).
The proof that this precategory is univalent (lemma \ref{thm:precat-univalent})
still needs a formalization.

\begin{leancode}
structure strict_precategory [class] (ob : Type) extends precategory ob :=
  (is_hset_ob : is_hset ob)

structure Strict_precategory : Type :=
  (carrier : Type)
  (struct : strict_precategory carrier)

definition precat_strict_precat : precategory Strict_precategory :=
precategory.mk (λ a b, functor a b)
  (λ a b, @functor.is_hset_functor a b _)
  (λ a b c g f, functor.compose g f)
  (λ a, functor.id)
  (λ a b c d h g f, !functor.assoc)
  (λ a b f, !functor.id_left)
  (λ a b f, !functor.id_right)
\end{leancode}

\begin{table}[h]
\begin{center}
\begin{tabular}{l|r|r}
\toprule[1pt]
\multicolumn{1}{c}{Theory} 
	& \multicolumn{1}{c}{Line Count} 
	& \multicolumn{1}{c}{Compilation Time in s} \\ 
\midrule[1pt]
\leani{algebra.} & & \\
	\hspace{1em}\leani{groupoid} & 119 & 0.419\\
	\hspace{1em}\leani{category.} & & \\
		\hspace{2em}\leani{basic} & 74 & 0.265 \\
		\hspace{2em}\leani{constructions} & 144 & 1.180 \\
	\hspace{1em}\leani{precategory.} &  & \\
		\hspace{2em}\leani{adjoints} & 143 & *0.638 \\
		\hspace{2em}\leani{basic} & 236 & 1.870 \\
		\hspace{2em}\leani{constructions} & 268 & 1.862 \\
		\hspace{2em}\leani{functor} & 253 & *2.981 \\
		\hspace{2em}\leani{iso} & 350 & 2.131 \\
		\hspace{2em}\leani{nat_trans} & 114 & 1.033 \\
		\hspace{2em}\leani{strict} & 53 & 0.246	\\
		\hspace{2em}\leani{yoneda} & 208 & 3.852 \\
\bottomrule[1pt]
\end{tabular}
\caption{The theories in Lean's category theory library.
Theories marked with * contain unfinished proofs.} \label{tab:cat-tree}
\end{center}
\end{table}

\section[Formalizing Double Groupoids and Crossed Modules]
	{Formalizing Double Groupoids and Crossed Modules
	\sectionmark{Formalization of DGpd and Xmod}}
\sectionmark{Formalization DGpd and Xmod}

When formalizing the structures presented in chapters \ref{chapter:nat} and
\ref{chapter:types}, I proceeded in the order the concepts are presented in this
thesis.
Since it was necessary to change several definitions to improve compatibility with
the library and to improve performance I had to change most of the definitions
repeatedly during the process.
In the following, I will only present the final version of the definitions.

The first structure is the one of a \textbf{double category}.
Since it has many fields, it was useful to come up with an idea to shorten the definition:
We first define what it means to be a \textbf{worm category}.
This structure consists of objects, morphisms and two-cells the same way a double
category does, but it only allows for composition in one direction.
In contrast to an actual double category there are essentially two, instead of
three, categories involved in the definition of a worm category.
\begin{leancodebr}
structure worm_precat [class] {D₀ : Type}
  (C  : precategory D₀)
  (D₂ : Π ⦃a b c d : D₀⦄
    (f : hom a b) (g : hom c d) (h : hom a c) (i : hom b d), Type) :=
  (comp₁ : proof Π ⦃a b c₁ d₁ c₂ d₂ : D₀⦄
    ⦃f₁ : hom a b⦄ ⦃g₁ : hom c₁ d₁⦄ ⦃h₁ : hom a c₁⦄ ⦃i₁ : hom b d₁⦄
    ⦃g₂ : hom c₂ d₂⦄ ⦃h₂ : hom c₁ c₂⦄ ⦃i₂ : hom d₁ d₂⦄,
    (D₂ g₁ g₂ h₂ i₂) → (D₂ f₁ g₁ h₁ i₁)
    → (@D₂ a b c₂ d₂ f₁ g₂ (h₂ ∘ h₁) (i₂ ∘ i₁)) qed)
  (ID₁ : proof Π ⦃a b : D₀⦄ (f : hom a b), D₂ f f (ID a) (ID b) qed)
  (assoc₁ : proof Π ⦃a b c₁ d₁ c₂ d₂ c₃ d₃ : D₀⦄
    ⦃f  : hom a b⦄   ⦃g₁ : hom c₁ d₁⦄ ⦃h₁ : hom a c₁⦄ ⦃i₁ : hom b d₁⦄
    ⦃g₂ : hom c₂ d₂⦄ ⦃h₂ : hom c₁ c₂⦄ ⦃i₂ : hom d₁ d₂⦄
    ⦃g₃ : hom c₃ d₃⦄ ⦃h₃ : hom c₂ c₃⦄ ⦃i₃ : hom d₂ d₃⦄
    (w : D₂ g₂ g₃ h₃ i₃) (v : D₂ g₁ g₂ h₂ i₂) (u : D₂ f g₁ h₁ i₁),
    (assoc i₃ i₂ i₁) ▹ ((assoc h₃ h₂ h₁) ▹
      (comp₁ w (comp₁ v u))) = (comp₁ (comp₁ w v) u) qed)
  (id_left₁ : proof Π ⦃a b c d : D₀⦄
    ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄
    (u : D₂ f g h i),
    (id_left i) ▹ ((id_left h) ▹ (comp₁ (ID₁ g) u)) = u qed)
  (id_right₁ : proof Π ⦃a b c d : D₀⦄
    ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄
    (u : D₂ f g h i),
    (id_right i) ▹ ((id_right h) ▹ (comp₁ u (ID₁ f))) = u qed)
  (homH' : proof Π ⦃a b c d : D₀⦄
    ⦃f : hom a b⦄ ⦃g : hom c d⦄ ⦃h : hom a c⦄ ⦃i : hom b d⦄,
    is_hset (D₂ f g h i) qed)
\end{leancodebr}

We then use the inheritance mechanism for structures to define a double category
as extending two worm precategories on the same object type \leani{D₀} and 1-skeleton
\leani{C} and a dependent type of two-cells that differ by \emph{transposition}
in the sense that if \leani{D₂} is the type of two-cells of the ``vertical''
worm category, \leani{(λ ⦃a b c d : D₀⦄ f g h i, D₂ h i f g)} is the respective
dependent type for the ``horizontal'' one.
To prevent the fields of the two worm precategories from being merged (which is
the default for structure fields with identical names), we have to rename
the fields of the horizontal worm category.
Then we add the laws that could not be expressed in terms of only one direction
of two-cell composition.
\begin{leancode}
structure dbl_precat [class] {D₀ : Type} (C : precategory D₀)
  (D₂ : Π ⦃a b c d : D₀⦄
    (f : hom a b) (g : hom c d) (h : hom a c) (i : hom b d), Type)
  extends worm_precat C D₂,
    worm_precat C (λ ⦃a b c d : D₀⦄ f g h i, D₂ h i f g)
    renaming comp₁→comp₂ ID₁→ID₂ assoc₁→assoc₂
      id_left₁→id_left₂ id_right₁→id_right₂ homH'→homH'_dontuse :=
  (id_comp₁ : proof Π {a b c : D₀} (f : hom a b) (g : hom b c),
    ID₂ (g ∘ f) = comp₁ (ID₂ g) (ID₂ f) qed)
  (id_comp₂ : proof Π {a b c : D₀} (f : hom a b) (g : hom b c),
    ID₁ (g ∘ f) = comp₂ (ID₁ g) (ID₁ f) qed)
  (zero_unique : proof Π (a : D₀), ID₁ (ID a) = ID₂ (ID a) qed)
  (interchange : proof Π {a₀₀ a₀₁ a₀₂ a₁₀ a₁₁ a₁₂ a₂₀ a₂₁ a₂₂ : D₀}
    {f₀₀ : hom a₀₀ a₀₁} {f₀₁ : hom a₀₁ a₀₂} {f₁₀ : hom a₁₀ a₁₁}
    {f₁₁ : hom a₁₁ a₁₂} {f₂₀ : hom a₂₀ a₂₁} {f₂₁ : hom a₂₁ a₂₂}
    {g₀₀ : hom a₀₀ a₁₀} {g₀₁ : hom a₀₁ a₁₁} {g₀₂ : hom a₀₂ a₁₂}
    {g₁₀ : hom a₁₀ a₂₀} {g₁₁ : hom a₁₁ a₂₁} {g₁₂ : hom a₁₂ a₂₂}
    (x : D₂ f₁₁ f₂₁ g₁₁ g₁₂) (w : D₂ f₁₀ f₂₀ g₁₀ g₁₁)
    (v : D₂ f₀₁ f₁₁ g₀₁ g₀₂) (u : D₂ f₀₀ f₁₀ g₀₀ g₀₁),
    comp₁ (comp₂ x w) (comp₂ v u) = comp₂ (comp₁ x v) (comp₁ w u) qed)
\end{leancode}

This unbundled definition of a precategory is useful when making statements about
all double category structures on a certain pair of 1-skeleton and two-cell type.
But since we often want to talk about \emph{all} double categories, we add a bundled
up version that also adds the strictness condition. %TODO make that consistent
\begin{leancode}
structure Dbl_precat : Type :=
  (cat : Precategory)
  (two_cell : Π ⦃a b c d : cat⦄ (f : hom a b)
    (g : hom c d) (h : hom a c) (i : hom b d), Type)
  (struct : dbl_precat cat two_cell)
  (obj_set : is_hset (carrier cat))

attribute Dbl_precat.struct [instance]
\end{leancode}

\section{Instantiating the Fundamental Double Groupoid}





